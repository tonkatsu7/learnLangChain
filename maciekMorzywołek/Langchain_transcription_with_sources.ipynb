{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonkatsu7/learnLangChain/blob/main/maciekMorzywo%C5%82ek/Langchain_transcription_with_sources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query the YouTube video transcripts, returning timestamps as sources to legitimize the answers by [@m_morzywolek](https://twitter.com/m_morzywolek)"
      ],
      "metadata": {
        "id": "6WFk81JVP4Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First set runtime to GPU"
      ],
      "metadata": {
        "id": "jdwAawIyJ6nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC0p1VRjIlFE",
        "outputId": "5b0ad728-b163-45a0-a7bd-120c0b9082b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install pytube # For audio downloading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/whisper.git -q # Whisper from OpenAI transcription model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zAC6DJ5IwTk",
        "outputId": "b86f4e52-d4f3-4ed9-8e23-6b2676425203"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper \n",
        "import pytube "
      ],
      "metadata": {
        "id": "8h_FeO8TI3Zn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.youtube.com/watch?v=UO699Szp82M\"\n",
        "video = pytube.YouTube(url)\n",
        "video.streams.get_highest_resolution().filesize"
      ],
      "metadata": {
        "id": "eOgbnvXkI50t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabf54e6-667a-465c-b90f-4f401bcb28c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22935284"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio = video.streams.get_audio_only()\n",
        "fn = audio.download(output_path=\"tmp.mp3\") # Downlods only audio from youtube video"
      ],
      "metadata": {
        "id": "ls2DYRxPJQmw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOZ6sSu1Jgm-",
        "outputId": "6acde969-6878-4eb5-e410-9f6cb3a9d7bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 176MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = model.transcribe('/content/tmp.mp3/LangChain In Action Real-World Use Case With Step-by-Step Tutorial.mp4')"
      ],
      "metadata": {
        "id": "DMCQql4AJmaf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcription"
      ],
      "metadata": {
        "id": "tAUMBq9nbYgW",
        "outputId": "19ed85fe-9851-419f-d783-d0325ed789d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': \" At the heart of the language model revolution and the chain framework lies the concept of a text embedding. A text embedding is a learned representation of text that takes the form of a vector of numbers. This vector allows us to efficiently prompt and retrieve context from vector storage to extract relevant pieces of information, enhance the language models memory and capabilities and ultimately take the action we want to take to generate value. In this video, we're going to look at this process by means of a real world practical application. We are going to use Langchain to extract information and value from Amazon Review Data. One of the most slam dunk applications of Langchain is custom experience and analytics. I'm going to show you how you can take the unstructured review data and map the reviews into themes and a structure that allows you to act on the data. I'm also going to demonstrate how the review embeddings can form the basis as inputs to other machine learning models and just how packed these reviews are with signal that can be used to further enhance the capabilities of the language models and avenue we're going to explore in more detail going forward on this channel. By the end of this video, you'll have an idea of how you can generate value for businesses with Langchain, how you combine vector stores with large language models and you'll have a small POC that you can further build on and put to good use. To get started, we'll pip install the needed libraries and we'll drop the needed API keys in variables into an environment file. Links to the code and the data will be available below the video. Let's go. So here we have the Amazon data that we're going to use. These are reviews for products in the fashion category. What's important to note here, there's an overall rating. There is a review text with the actual review from the customer and there's an ID that allows us to try this to a specific product and then we have a complimentary data set called meta-amazon fashion with all the product information and an ID that allows us to join this with the review data set. In our notebook, the first thing we're going to do is we're going to import the utilities needed for loading the API keys from the environment file and also extract the Amazon data from the files. And we're actually not going to extract the data from the JSON files as they are quite big, but directly from the SIPT files. Then I'm going to load both data sets into pandas data frames and I'm removing reviews without a review text as we need the text for the work in this video. And here we have the review data in a data frame. And this is the metadata also in a data frame. Next, I'm going to trunk it reviews so that we don't process reviews that are too long and you can play around with the number of characters you want to use. And then I'm going to find a product that has a good number of reviews for the sake of this video. And I had a look at the data and it looks like the second one from the bottom is a good fit. And if we extract that product, you can see the name it's called power step pinnacle, authentic shoe insoles. I guess that's fashion, but this is good for our purpose. So let's create the embedding vectors from these reviews. So we're going to work on just a slice of the data frame with the power step pinnacle insoles. And as for embeddings, I'm going to use hucking phase embeddings. And this is just to show that we don't have to use openAI for this. I'm going to create a new column in the data frame with the embedding vectors. So I'm going to use the apply function on the data frame and the word of caution here. Another reason I'm not using the openAI embeddings is because of the openAI pricing model. When you're running apply with an embedding function on a large data frame, you risk incurring significant costs using openAI. Here we're only working on a slice of the original full data frame, but the full data frame has more than 800,000 rows. So please don't run the apply function with an openAI embedding unless you know what you're doing. All right, here we have the embedding vectors as a new column in the data frame. And what I'm going to do now is I'm going to show you the richness of this review data. And I'll do that by training a simple random forest machine learning model with the embedding vectors as features and the overall rating as a target. I'll use scikit-learn to divide the data into a training sample and a test sample. And then I'm going to import a random forest regressor. So I'm treating this as a regression problem, meaning that the prediction is on a continuum, even though that we know that the rating is an integer. But this is fine for demonstration. And then I'm training the model on the training part of the data set. And here you can also play around with the number of estimators you're using. I found that 150 was fine to make this point. And once the fitting is done, we can evaluate by using the mean absolute error on the test part of the data set. And we see that we achieve a mean absolute error of 0.53, which means that on average, the prediction is off by around half a point. And this is with a simple non-optimized machine learning model that takes five minutes to run. If you spend some time optimizing this, are building a more advanced model in PyTorch, you could probably get to 0.3 or below. So there is significant signal in this data. But why would you want to predict the rating? Well, you wouldn't unless you're missing the rating for some of the reviews. What you want to do is to switch out the target variable and use the signal in the review data to build machine learning models that will actually help you generate value. And these are product recommendation models, churn or retention models, propensity models, uplift models, and so on. I'm now going to load the review embeddings into the vector database and show you how we can have GBT for access the data and give us a summary of the reviews. I'm using GBT for as it's currently the most powerful language model we have to work with, but you can switch out the language model as you see fit. To upload the review embeddings, we import and initiate pine cone and then transform the truncated review text column into a list of reviews. Then we upload the reviews with the built-in from text method using Hocking Face embeddings. Once the upload is done, we can head over to pine cone and check that the vectors have been uploaded. And here we have all the review vectors uploaded into pine cone. And then we can do a basic vector similarity search to check that everything works. What I want to do now is I want to have the language model access the data in the vector store. So I'm going to import retrieval QA that is used to retrieve the most relevant reviews given a prompt and feed those to the language model. And then I'm going to import chat.obmAPI that I'm going to use as a driver around GBT for. Then we're going to define a chain. It's called review chain using retrieval QA that takes the language model, the vector store, and then the chain type as an argument. And here we use chain type stuff, which means that we stuff all the related data into the prompt and we use that as a context and pass it to the language model. Then we simply write the query as we usually do when working with chat.obt and we run the chain with this query. And here I'm asking to be for to give us an overall impression of the reviews and give us the most prevalent examples and bullet points and also give us suggestions for improvement. And remember when you're working with chat models like DBT4 you can send system messages that will allow you to calibrate the system of the model, which could significantly improve the quality of the output. Alright so here we are with DBT4's impression of the reviews. Some examples of good reviews, mixed reviews, some negative reviews, and then based on those reviews there are some potential areas to focus on improving. So pretty nice and I'm sure you can improve that by calibrating the system. What's really powerful about this is that you can turn this into a weekly digest and send it out to an internal email list within a company and this can also be done with chain. Alright so the last thing we're going to have a look at is filtered vector similar research and what I'd like to do is to get all the reviews of a given rating that match a specific theme so we need to filter on metadata to do that. I'm going to use Pinecon directly as Langchain doesn't seem to support this out of the box yet. To upload the data with the Pinecon Python client I'm going to add a metadata field to the data frame and create two versions of the data frame. One for uploading and a local version for extracting the actual reviews. Then I'm going to create the Pinecon index directly with the Pinecon Python client. And while the index is initializing we can head over to the Pinecon documentation and have a look at what the absurd should look like. So we can see that we need an ID field and a value field with the vector values and then we're going to add the metadata field used for filtering. Once the index has been initialized we can upload the data in patches and here I'm using patches of 50. All right now that the upload is done let's try to run a filtered query. We do that by using a query string like we do without the filter and then add a filter that has a MongoDB like syntax. This query will give us the top 100 matching reviews with a rating of 4. And the result is a list of ITs with a score indicating the match rate. To get the actual reviews we'll need to use the local version of the data. And what I'm going to do is I'm going to wrap the query in a Python function that executes a filtered vector similarity search followed by looping through the local version of the data and extracting all the reviews from the local data. Now if I call this function with the query will purchase again filtering out the ratings with five star reviews then we get all the happy customers that should be put in a repeat purchase flow. And we can do something similar to get all the disappointed customers that give a one star review which works even though I misspelled disappointed and we can use this in a winback flow. Being able to filter out reviews like this is very powerful as it allows you to divide reviews into themes and get the angles you need in your remarketing campaigns. In the example we just saw on the notebook we have two different themes we have the disappointed customers and we have the customers that want to purchase again. And if you have a custom ID and an email tied to the customer review you can use Lanchions integration with Sepia to create two lists a winback list and a repurchase list then use your email service provider to run two different campaigns. That's a topic for another video. So that's it for now if you enjoyed this video give it a like and subscribe thanks for watching.\",\n",
              " 'segments': [{'id': 0,\n",
              "   'seek': 0,\n",
              "   'start': 0.0,\n",
              "   'end': 6.36,\n",
              "   'text': ' At the heart of the language model revolution and the chain framework lies the concept of a text embedding.',\n",
              "   'tokens': [50364,\n",
              "    1711,\n",
              "    264,\n",
              "    1917,\n",
              "    295,\n",
              "    264,\n",
              "    2856,\n",
              "    2316,\n",
              "    8894,\n",
              "    293,\n",
              "    264,\n",
              "    5021,\n",
              "    8388,\n",
              "    9134,\n",
              "    264,\n",
              "    3410,\n",
              "    295,\n",
              "    257,\n",
              "    2487,\n",
              "    12240,\n",
              "    3584,\n",
              "    13,\n",
              "    50682],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22867914425429478,\n",
              "   'compression_ratio': 1.7578125,\n",
              "   'no_speech_prob': 0.035613663494586945},\n",
              "  {'id': 1,\n",
              "   'seek': 0,\n",
              "   'start': 6.36,\n",
              "   'end': 11.76,\n",
              "   'text': ' A text embedding is a learned representation of text that takes the form of a vector of numbers.',\n",
              "   'tokens': [50682,\n",
              "    316,\n",
              "    2487,\n",
              "    12240,\n",
              "    3584,\n",
              "    307,\n",
              "    257,\n",
              "    3264,\n",
              "    10290,\n",
              "    295,\n",
              "    2487,\n",
              "    300,\n",
              "    2516,\n",
              "    264,\n",
              "    1254,\n",
              "    295,\n",
              "    257,\n",
              "    8062,\n",
              "    295,\n",
              "    3547,\n",
              "    13,\n",
              "    50952],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22867914425429478,\n",
              "   'compression_ratio': 1.7578125,\n",
              "   'no_speech_prob': 0.035613663494586945},\n",
              "  {'id': 2,\n",
              "   'seek': 0,\n",
              "   'start': 11.76,\n",
              "   'end': 17.16,\n",
              "   'text': ' This vector allows us to efficiently prompt and retrieve context from vector storage',\n",
              "   'tokens': [50952,\n",
              "    639,\n",
              "    8062,\n",
              "    4045,\n",
              "    505,\n",
              "    281,\n",
              "    19621,\n",
              "    12391,\n",
              "    293,\n",
              "    30254,\n",
              "    4319,\n",
              "    490,\n",
              "    8062,\n",
              "    6725,\n",
              "    51222],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22867914425429478,\n",
              "   'compression_ratio': 1.7578125,\n",
              "   'no_speech_prob': 0.035613663494586945},\n",
              "  {'id': 3,\n",
              "   'seek': 0,\n",
              "   'start': 17.16,\n",
              "   'end': 22.56,\n",
              "   'text': ' to extract relevant pieces of information, enhance the language models memory and capabilities',\n",
              "   'tokens': [51222,\n",
              "    281,\n",
              "    8947,\n",
              "    7340,\n",
              "    3755,\n",
              "    295,\n",
              "    1589,\n",
              "    11,\n",
              "    11985,\n",
              "    264,\n",
              "    2856,\n",
              "    5245,\n",
              "    4675,\n",
              "    293,\n",
              "    10862,\n",
              "    51492],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22867914425429478,\n",
              "   'compression_ratio': 1.7578125,\n",
              "   'no_speech_prob': 0.035613663494586945},\n",
              "  {'id': 4,\n",
              "   'seek': 0,\n",
              "   'start': 22.56,\n",
              "   'end': 26.04,\n",
              "   'text': ' and ultimately take the action we want to take to generate value.',\n",
              "   'tokens': [51492,\n",
              "    293,\n",
              "    6284,\n",
              "    747,\n",
              "    264,\n",
              "    3069,\n",
              "    321,\n",
              "    528,\n",
              "    281,\n",
              "    747,\n",
              "    281,\n",
              "    8460,\n",
              "    2158,\n",
              "    13,\n",
              "    51666],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.22867914425429478,\n",
              "   'compression_ratio': 1.7578125,\n",
              "   'no_speech_prob': 0.035613663494586945},\n",
              "  {'id': 5,\n",
              "   'seek': 2604,\n",
              "   'start': 26.2,\n",
              "   'end': 31.08,\n",
              "   'text': \" In this video, we're going to look at this process by means of a real world practical application.\",\n",
              "   'tokens': [50372,\n",
              "    682,\n",
              "    341,\n",
              "    960,\n",
              "    11,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    574,\n",
              "    412,\n",
              "    341,\n",
              "    1399,\n",
              "    538,\n",
              "    1355,\n",
              "    295,\n",
              "    257,\n",
              "    957,\n",
              "    1002,\n",
              "    8496,\n",
              "    3861,\n",
              "    13,\n",
              "    50616],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18319828227414922,\n",
              "   'compression_ratio': 1.768166089965398,\n",
              "   'no_speech_prob': 0.0012627174146473408},\n",
              "  {'id': 6,\n",
              "   'seek': 2604,\n",
              "   'start': 31.08,\n",
              "   'end': 36.519999999999996,\n",
              "   'text': ' We are going to use Langchain to extract information and value from Amazon Review Data.',\n",
              "   'tokens': [50616,\n",
              "    492,\n",
              "    366,\n",
              "    516,\n",
              "    281,\n",
              "    764,\n",
              "    13313,\n",
              "    11509,\n",
              "    281,\n",
              "    8947,\n",
              "    1589,\n",
              "    293,\n",
              "    2158,\n",
              "    490,\n",
              "    6795,\n",
              "    19954,\n",
              "    11888,\n",
              "    13,\n",
              "    50888],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18319828227414922,\n",
              "   'compression_ratio': 1.768166089965398,\n",
              "   'no_speech_prob': 0.0012627174146473408},\n",
              "  {'id': 7,\n",
              "   'seek': 2604,\n",
              "   'start': 36.519999999999996,\n",
              "   'end': 41.24,\n",
              "   'text': ' One of the most slam dunk applications of Langchain is custom experience and analytics.',\n",
              "   'tokens': [50888,\n",
              "    1485,\n",
              "    295,\n",
              "    264,\n",
              "    881,\n",
              "    25617,\n",
              "    33555,\n",
              "    5821,\n",
              "    295,\n",
              "    13313,\n",
              "    11509,\n",
              "    307,\n",
              "    2375,\n",
              "    1752,\n",
              "    293,\n",
              "    15370,\n",
              "    13,\n",
              "    51124],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18319828227414922,\n",
              "   'compression_ratio': 1.768166089965398,\n",
              "   'no_speech_prob': 0.0012627174146473408},\n",
              "  {'id': 8,\n",
              "   'seek': 2604,\n",
              "   'start': 41.24,\n",
              "   'end': 46.68,\n",
              "   'text': \" I'm going to show you how you can take the unstructured review data and map the reviews into themes\",\n",
              "   'tokens': [51124,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    855,\n",
              "    291,\n",
              "    577,\n",
              "    291,\n",
              "    393,\n",
              "    747,\n",
              "    264,\n",
              "    18799,\n",
              "    46847,\n",
              "    3131,\n",
              "    1412,\n",
              "    293,\n",
              "    4471,\n",
              "    264,\n",
              "    10229,\n",
              "    666,\n",
              "    13544,\n",
              "    51396],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18319828227414922,\n",
              "   'compression_ratio': 1.768166089965398,\n",
              "   'no_speech_prob': 0.0012627174146473408},\n",
              "  {'id': 9,\n",
              "   'seek': 2604,\n",
              "   'start': 46.68,\n",
              "   'end': 50.120000000000005,\n",
              "   'text': ' and a structure that allows you to act on the data.',\n",
              "   'tokens': [51396,\n",
              "    293,\n",
              "    257,\n",
              "    3877,\n",
              "    300,\n",
              "    4045,\n",
              "    291,\n",
              "    281,\n",
              "    605,\n",
              "    322,\n",
              "    264,\n",
              "    1412,\n",
              "    13,\n",
              "    51568],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18319828227414922,\n",
              "   'compression_ratio': 1.768166089965398,\n",
              "   'no_speech_prob': 0.0012627174146473408},\n",
              "  {'id': 10,\n",
              "   'seek': 2604,\n",
              "   'start': 50.120000000000005,\n",
              "   'end': 54.44,\n",
              "   'text': \" I'm also going to demonstrate how the review embeddings can form the basis as inputs\",\n",
              "   'tokens': [51568,\n",
              "    286,\n",
              "    478,\n",
              "    611,\n",
              "    516,\n",
              "    281,\n",
              "    11698,\n",
              "    577,\n",
              "    264,\n",
              "    3131,\n",
              "    12240,\n",
              "    29432,\n",
              "    393,\n",
              "    1254,\n",
              "    264,\n",
              "    5143,\n",
              "    382,\n",
              "    15743,\n",
              "    51784],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18319828227414922,\n",
              "   'compression_ratio': 1.768166089965398,\n",
              "   'no_speech_prob': 0.0012627174146473408},\n",
              "  {'id': 11,\n",
              "   'seek': 5444,\n",
              "   'start': 54.44,\n",
              "   'end': 59.239999999999995,\n",
              "   'text': ' to other machine learning models and just how packed these reviews are with signal',\n",
              "   'tokens': [50364,\n",
              "    281,\n",
              "    661,\n",
              "    3479,\n",
              "    2539,\n",
              "    5245,\n",
              "    293,\n",
              "    445,\n",
              "    577,\n",
              "    13265,\n",
              "    613,\n",
              "    10229,\n",
              "    366,\n",
              "    365,\n",
              "    6358,\n",
              "    50604],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11493920396875452,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.0007665363373234868},\n",
              "  {'id': 12,\n",
              "   'seek': 5444,\n",
              "   'start': 59.239999999999995,\n",
              "   'end': 64.12,\n",
              "   'text': \" that can be used to further enhance the capabilities of the language models and avenue we're going to\",\n",
              "   'tokens': [50604,\n",
              "    300,\n",
              "    393,\n",
              "    312,\n",
              "    1143,\n",
              "    281,\n",
              "    3052,\n",
              "    11985,\n",
              "    264,\n",
              "    10862,\n",
              "    295,\n",
              "    264,\n",
              "    2856,\n",
              "    5245,\n",
              "    293,\n",
              "    39230,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    50848],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11493920396875452,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.0007665363373234868},\n",
              "  {'id': 13,\n",
              "   'seek': 5444,\n",
              "   'start': 64.12,\n",
              "   'end': 70.84,\n",
              "   'text': \" explore in more detail going forward on this channel. By the end of this video, you'll have an idea\",\n",
              "   'tokens': [50848,\n",
              "    6839,\n",
              "    294,\n",
              "    544,\n",
              "    2607,\n",
              "    516,\n",
              "    2128,\n",
              "    322,\n",
              "    341,\n",
              "    2269,\n",
              "    13,\n",
              "    3146,\n",
              "    264,\n",
              "    917,\n",
              "    295,\n",
              "    341,\n",
              "    960,\n",
              "    11,\n",
              "    291,\n",
              "    603,\n",
              "    362,\n",
              "    364,\n",
              "    1558,\n",
              "    51184],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11493920396875452,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.0007665363373234868},\n",
              "  {'id': 14,\n",
              "   'seek': 5444,\n",
              "   'start': 70.84,\n",
              "   'end': 76.52,\n",
              "   'text': ' of how you can generate value for businesses with Langchain, how you combine vector stores with',\n",
              "   'tokens': [51184,\n",
              "    295,\n",
              "    577,\n",
              "    291,\n",
              "    393,\n",
              "    8460,\n",
              "    2158,\n",
              "    337,\n",
              "    6011,\n",
              "    365,\n",
              "    13313,\n",
              "    11509,\n",
              "    11,\n",
              "    577,\n",
              "    291,\n",
              "    10432,\n",
              "    8062,\n",
              "    9512,\n",
              "    365,\n",
              "    51468],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11493920396875452,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.0007665363373234868},\n",
              "  {'id': 15,\n",
              "   'seek': 5444,\n",
              "   'start': 76.52,\n",
              "   'end': 81.64,\n",
              "   'text': \" large language models and you'll have a small POC that you can further build on and put to good use.\",\n",
              "   'tokens': [51468,\n",
              "    2416,\n",
              "    2856,\n",
              "    5245,\n",
              "    293,\n",
              "    291,\n",
              "    603,\n",
              "    362,\n",
              "    257,\n",
              "    1359,\n",
              "    22299,\n",
              "    34,\n",
              "    300,\n",
              "    291,\n",
              "    393,\n",
              "    3052,\n",
              "    1322,\n",
              "    322,\n",
              "    293,\n",
              "    829,\n",
              "    281,\n",
              "    665,\n",
              "    764,\n",
              "    13,\n",
              "    51724],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11493920396875452,\n",
              "   'compression_ratio': 1.7619047619047619,\n",
              "   'no_speech_prob': 0.0007665363373234868},\n",
              "  {'id': 16,\n",
              "   'seek': 8164,\n",
              "   'start': 82.12,\n",
              "   'end': 88.04,\n",
              "   'text': \" To get started, we'll pip install the needed libraries and we'll drop the needed API keys in\",\n",
              "   'tokens': [50388,\n",
              "    1407,\n",
              "    483,\n",
              "    1409,\n",
              "    11,\n",
              "    321,\n",
              "    603,\n",
              "    8489,\n",
              "    3625,\n",
              "    264,\n",
              "    2978,\n",
              "    15148,\n",
              "    293,\n",
              "    321,\n",
              "    603,\n",
              "    3270,\n",
              "    264,\n",
              "    2978,\n",
              "    9362,\n",
              "    9317,\n",
              "    294,\n",
              "    50684],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13255816317619162,\n",
              "   'compression_ratio': 1.5755102040816327,\n",
              "   'no_speech_prob': 0.007228231988847256},\n",
              "  {'id': 17,\n",
              "   'seek': 8164,\n",
              "   'start': 88.04,\n",
              "   'end': 94.12,\n",
              "   'text': ' variables into an environment file. Links to the code and the data will be available below the video.',\n",
              "   'tokens': [50684,\n",
              "    9102,\n",
              "    666,\n",
              "    364,\n",
              "    2823,\n",
              "    3991,\n",
              "    13,\n",
              "    37156,\n",
              "    281,\n",
              "    264,\n",
              "    3089,\n",
              "    293,\n",
              "    264,\n",
              "    1412,\n",
              "    486,\n",
              "    312,\n",
              "    2435,\n",
              "    2507,\n",
              "    264,\n",
              "    960,\n",
              "    13,\n",
              "    50988],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13255816317619162,\n",
              "   'compression_ratio': 1.5755102040816327,\n",
              "   'no_speech_prob': 0.007228231988847256},\n",
              "  {'id': 18,\n",
              "   'seek': 8164,\n",
              "   'start': 94.12,\n",
              "   'end': 101.72,\n",
              "   'text': \" Let's go. So here we have the Amazon data that we're going to use. These are reviews for products\",\n",
              "   'tokens': [50988,\n",
              "    961,\n",
              "    311,\n",
              "    352,\n",
              "    13,\n",
              "    407,\n",
              "    510,\n",
              "    321,\n",
              "    362,\n",
              "    264,\n",
              "    6795,\n",
              "    1412,\n",
              "    300,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    764,\n",
              "    13,\n",
              "    1981,\n",
              "    366,\n",
              "    10229,\n",
              "    337,\n",
              "    3383,\n",
              "    51368],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13255816317619162,\n",
              "   'compression_ratio': 1.5755102040816327,\n",
              "   'no_speech_prob': 0.007228231988847256},\n",
              "  {'id': 19,\n",
              "   'seek': 8164,\n",
              "   'start': 101.72,\n",
              "   'end': 107.96000000000001,\n",
              "   'text': \" in the fashion category. What's important to note here, there's an overall rating. There is a\",\n",
              "   'tokens': [51368,\n",
              "    294,\n",
              "    264,\n",
              "    6700,\n",
              "    7719,\n",
              "    13,\n",
              "    708,\n",
              "    311,\n",
              "    1021,\n",
              "    281,\n",
              "    3637,\n",
              "    510,\n",
              "    11,\n",
              "    456,\n",
              "    311,\n",
              "    364,\n",
              "    4787,\n",
              "    10990,\n",
              "    13,\n",
              "    821,\n",
              "    307,\n",
              "    257,\n",
              "    51680],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13255816317619162,\n",
              "   'compression_ratio': 1.5755102040816327,\n",
              "   'no_speech_prob': 0.007228231988847256},\n",
              "  {'id': 20,\n",
              "   'seek': 10796,\n",
              "   'start': 108.03999999999999,\n",
              "   'end': 112.83999999999999,\n",
              "   'text': \" review text with the actual review from the customer and there's an ID that allows us to\",\n",
              "   'tokens': [50368,\n",
              "    3131,\n",
              "    2487,\n",
              "    365,\n",
              "    264,\n",
              "    3539,\n",
              "    3131,\n",
              "    490,\n",
              "    264,\n",
              "    5474,\n",
              "    293,\n",
              "    456,\n",
              "    311,\n",
              "    364,\n",
              "    7348,\n",
              "    300,\n",
              "    4045,\n",
              "    505,\n",
              "    281,\n",
              "    50608],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14304458961058197,\n",
              "   'compression_ratio': 1.7767441860465116,\n",
              "   'no_speech_prob': 0.0013880535261705518},\n",
              "  {'id': 21,\n",
              "   'seek': 10796,\n",
              "   'start': 112.83999999999999,\n",
              "   'end': 119.08,\n",
              "   'text': ' try this to a specific product and then we have a complimentary data set called meta-amazon fashion',\n",
              "   'tokens': [50608,\n",
              "    853,\n",
              "    341,\n",
              "    281,\n",
              "    257,\n",
              "    2685,\n",
              "    1674,\n",
              "    293,\n",
              "    550,\n",
              "    321,\n",
              "    362,\n",
              "    257,\n",
              "    47162,\n",
              "    1412,\n",
              "    992,\n",
              "    1219,\n",
              "    19616,\n",
              "    12,\n",
              "    335,\n",
              "    6317,\n",
              "    6700,\n",
              "    50920],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14304458961058197,\n",
              "   'compression_ratio': 1.7767441860465116,\n",
              "   'no_speech_prob': 0.0013880535261705518},\n",
              "  {'id': 22,\n",
              "   'seek': 10796,\n",
              "   'start': 119.08,\n",
              "   'end': 125.32,\n",
              "   'text': ' with all the product information and an ID that allows us to join this with the review data set.',\n",
              "   'tokens': [50920,\n",
              "    365,\n",
              "    439,\n",
              "    264,\n",
              "    1674,\n",
              "    1589,\n",
              "    293,\n",
              "    364,\n",
              "    7348,\n",
              "    300,\n",
              "    4045,\n",
              "    505,\n",
              "    281,\n",
              "    3917,\n",
              "    341,\n",
              "    365,\n",
              "    264,\n",
              "    3131,\n",
              "    1412,\n",
              "    992,\n",
              "    13,\n",
              "    51232],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14304458961058197,\n",
              "   'compression_ratio': 1.7767441860465116,\n",
              "   'no_speech_prob': 0.0013880535261705518},\n",
              "  {'id': 23,\n",
              "   'seek': 10796,\n",
              "   'start': 127.56,\n",
              "   'end': 131.56,\n",
              "   'text': \" In our notebook, the first thing we're going to do is we're going to import the utilities needed\",\n",
              "   'tokens': [51344,\n",
              "    682,\n",
              "    527,\n",
              "    21060,\n",
              "    11,\n",
              "    264,\n",
              "    700,\n",
              "    551,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    360,\n",
              "    307,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    974,\n",
              "    264,\n",
              "    30482,\n",
              "    2978,\n",
              "    51544],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14304458961058197,\n",
              "   'compression_ratio': 1.7767441860465116,\n",
              "   'no_speech_prob': 0.0013880535261705518},\n",
              "  {'id': 24,\n",
              "   'seek': 13156,\n",
              "   'start': 131.56,\n",
              "   'end': 137.48,\n",
              "   'text': ' for loading the API keys from the environment file and also extract the Amazon data from the files.',\n",
              "   'tokens': [50364,\n",
              "    337,\n",
              "    15114,\n",
              "    264,\n",
              "    9362,\n",
              "    9317,\n",
              "    490,\n",
              "    264,\n",
              "    2823,\n",
              "    3991,\n",
              "    293,\n",
              "    611,\n",
              "    8947,\n",
              "    264,\n",
              "    6795,\n",
              "    1412,\n",
              "    490,\n",
              "    264,\n",
              "    7098,\n",
              "    13,\n",
              "    50660],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14952527323076803,\n",
              "   'compression_ratio': 1.6828193832599119,\n",
              "   'no_speech_prob': 0.000385817838832736},\n",
              "  {'id': 25,\n",
              "   'seek': 13156,\n",
              "   'start': 138.68,\n",
              "   'end': 142.36,\n",
              "   'text': \" And we're actually not going to extract the data from the JSON files as they are quite big,\",\n",
              "   'tokens': [50720,\n",
              "    400,\n",
              "    321,\n",
              "    434,\n",
              "    767,\n",
              "    406,\n",
              "    516,\n",
              "    281,\n",
              "    8947,\n",
              "    264,\n",
              "    1412,\n",
              "    490,\n",
              "    264,\n",
              "    31828,\n",
              "    7098,\n",
              "    382,\n",
              "    436,\n",
              "    366,\n",
              "    1596,\n",
              "    955,\n",
              "    11,\n",
              "    50904],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14952527323076803,\n",
              "   'compression_ratio': 1.6828193832599119,\n",
              "   'no_speech_prob': 0.000385817838832736},\n",
              "  {'id': 26,\n",
              "   'seek': 13156,\n",
              "   'start': 142.36,\n",
              "   'end': 149.32,\n",
              "   'text': \" but directly from the SIPT files. Then I'm going to load both data sets into pandas data frames\",\n",
              "   'tokens': [50904,\n",
              "    457,\n",
              "    3838,\n",
              "    490,\n",
              "    264,\n",
              "    318,\n",
              "    9139,\n",
              "    51,\n",
              "    7098,\n",
              "    13,\n",
              "    1396,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    3677,\n",
              "    1293,\n",
              "    1412,\n",
              "    6352,\n",
              "    666,\n",
              "    4565,\n",
              "    296,\n",
              "    1412,\n",
              "    12083,\n",
              "    51252],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14952527323076803,\n",
              "   'compression_ratio': 1.6828193832599119,\n",
              "   'no_speech_prob': 0.000385817838832736},\n",
              "  {'id': 27,\n",
              "   'seek': 13156,\n",
              "   'start': 149.32,\n",
              "   'end': 154.84,\n",
              "   'text': \" and I'm removing reviews without a review text as we need the text for the work in this video.\",\n",
              "   'tokens': [51252,\n",
              "    293,\n",
              "    286,\n",
              "    478,\n",
              "    12720,\n",
              "    10229,\n",
              "    1553,\n",
              "    257,\n",
              "    3131,\n",
              "    2487,\n",
              "    382,\n",
              "    321,\n",
              "    643,\n",
              "    264,\n",
              "    2487,\n",
              "    337,\n",
              "    264,\n",
              "    589,\n",
              "    294,\n",
              "    341,\n",
              "    960,\n",
              "    13,\n",
              "    51528],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.14952527323076803,\n",
              "   'compression_ratio': 1.6828193832599119,\n",
              "   'no_speech_prob': 0.000385817838832736},\n",
              "  {'id': 28,\n",
              "   'seek': 15484,\n",
              "   'start': 154.84,\n",
              "   'end': 159.88,\n",
              "   'text': ' And here we have the review data in a data frame.',\n",
              "   'tokens': [50364,\n",
              "    400,\n",
              "    510,\n",
              "    321,\n",
              "    362,\n",
              "    264,\n",
              "    3131,\n",
              "    1412,\n",
              "    294,\n",
              "    257,\n",
              "    1412,\n",
              "    3920,\n",
              "    13,\n",
              "    50616],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15865098802666916,\n",
              "   'compression_ratio': 1.6264367816091954,\n",
              "   'no_speech_prob': 0.005205330904573202},\n",
              "  {'id': 29,\n",
              "   'seek': 15484,\n",
              "   'start': 164.2,\n",
              "   'end': 167.24,\n",
              "   'text': ' And this is the metadata also in a data frame.',\n",
              "   'tokens': [50832,\n",
              "    400,\n",
              "    341,\n",
              "    307,\n",
              "    264,\n",
              "    26603,\n",
              "    611,\n",
              "    294,\n",
              "    257,\n",
              "    1412,\n",
              "    3920,\n",
              "    13,\n",
              "    50984],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15865098802666916,\n",
              "   'compression_ratio': 1.6264367816091954,\n",
              "   'no_speech_prob': 0.005205330904573202},\n",
              "  {'id': 30,\n",
              "   'seek': 15484,\n",
              "   'start': 171.4,\n",
              "   'end': 176.2,\n",
              "   'text': \" Next, I'm going to trunk it reviews so that we don't process reviews that are too long and you\",\n",
              "   'tokens': [51192,\n",
              "    3087,\n",
              "    11,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    19849,\n",
              "    309,\n",
              "    10229,\n",
              "    370,\n",
              "    300,\n",
              "    321,\n",
              "    500,\n",
              "    380,\n",
              "    1399,\n",
              "    10229,\n",
              "    300,\n",
              "    366,\n",
              "    886,\n",
              "    938,\n",
              "    293,\n",
              "    291,\n",
              "    51432],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15865098802666916,\n",
              "   'compression_ratio': 1.6264367816091954,\n",
              "   'no_speech_prob': 0.005205330904573202},\n",
              "  {'id': 31,\n",
              "   'seek': 15484,\n",
              "   'start': 176.2,\n",
              "   'end': 181.48000000000002,\n",
              "   'text': \" can play around with the number of characters you want to use. And then I'm going to find a\",\n",
              "   'tokens': [51432,\n",
              "    393,\n",
              "    862,\n",
              "    926,\n",
              "    365,\n",
              "    264,\n",
              "    1230,\n",
              "    295,\n",
              "    4342,\n",
              "    291,\n",
              "    528,\n",
              "    281,\n",
              "    764,\n",
              "    13,\n",
              "    400,\n",
              "    550,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    915,\n",
              "    257,\n",
              "    51696],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15865098802666916,\n",
              "   'compression_ratio': 1.6264367816091954,\n",
              "   'no_speech_prob': 0.005205330904573202},\n",
              "  {'id': 32,\n",
              "   'seek': 18148,\n",
              "   'start': 181.48,\n",
              "   'end': 186.35999999999999,\n",
              "   'text': ' product that has a good number of reviews for the sake of this video. And I had a look at the',\n",
              "   'tokens': [50364,\n",
              "    1674,\n",
              "    300,\n",
              "    575,\n",
              "    257,\n",
              "    665,\n",
              "    1230,\n",
              "    295,\n",
              "    10229,\n",
              "    337,\n",
              "    264,\n",
              "    9717,\n",
              "    295,\n",
              "    341,\n",
              "    960,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    632,\n",
              "    257,\n",
              "    574,\n",
              "    412,\n",
              "    264,\n",
              "    50608],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15770720481872558,\n",
              "   'compression_ratio': 1.6581196581196582,\n",
              "   'no_speech_prob': 0.0012441660510376096},\n",
              "  {'id': 33,\n",
              "   'seek': 18148,\n",
              "   'start': 186.35999999999999,\n",
              "   'end': 193.39999999999998,\n",
              "   'text': ' data and it looks like the second one from the bottom is a good fit. And if we extract that product,',\n",
              "   'tokens': [50608,\n",
              "    1412,\n",
              "    293,\n",
              "    309,\n",
              "    1542,\n",
              "    411,\n",
              "    264,\n",
              "    1150,\n",
              "    472,\n",
              "    490,\n",
              "    264,\n",
              "    2767,\n",
              "    307,\n",
              "    257,\n",
              "    665,\n",
              "    3318,\n",
              "    13,\n",
              "    400,\n",
              "    498,\n",
              "    321,\n",
              "    8947,\n",
              "    300,\n",
              "    1674,\n",
              "    11,\n",
              "    50960],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15770720481872558,\n",
              "   'compression_ratio': 1.6581196581196582,\n",
              "   'no_speech_prob': 0.0012441660510376096},\n",
              "  {'id': 34,\n",
              "   'seek': 18148,\n",
              "   'start': 193.39999999999998,\n",
              "   'end': 199.64,\n",
              "   'text': \" you can see the name it's called power step pinnacle, authentic shoe insoles. I guess that's\",\n",
              "   'tokens': [50960,\n",
              "    291,\n",
              "    393,\n",
              "    536,\n",
              "    264,\n",
              "    1315,\n",
              "    309,\n",
              "    311,\n",
              "    1219,\n",
              "    1347,\n",
              "    1823,\n",
              "    5447,\n",
              "    77,\n",
              "    7041,\n",
              "    11,\n",
              "    12466,\n",
              "    12796,\n",
              "    1028,\n",
              "    7456,\n",
              "    13,\n",
              "    286,\n",
              "    2041,\n",
              "    300,\n",
              "    311,\n",
              "    51272],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15770720481872558,\n",
              "   'compression_ratio': 1.6581196581196582,\n",
              "   'no_speech_prob': 0.0012441660510376096},\n",
              "  {'id': 35,\n",
              "   'seek': 18148,\n",
              "   'start': 199.64,\n",
              "   'end': 206.83999999999997,\n",
              "   'text': \" fashion, but this is good for our purpose. So let's create the embedding vectors from these reviews.\",\n",
              "   'tokens': [51272,\n",
              "    6700,\n",
              "    11,\n",
              "    457,\n",
              "    341,\n",
              "    307,\n",
              "    665,\n",
              "    337,\n",
              "    527,\n",
              "    4334,\n",
              "    13,\n",
              "    407,\n",
              "    718,\n",
              "    311,\n",
              "    1884,\n",
              "    264,\n",
              "    12240,\n",
              "    3584,\n",
              "    18875,\n",
              "    490,\n",
              "    613,\n",
              "    10229,\n",
              "    13,\n",
              "    51632],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.15770720481872558,\n",
              "   'compression_ratio': 1.6581196581196582,\n",
              "   'no_speech_prob': 0.0012441660510376096},\n",
              "  {'id': 36,\n",
              "   'seek': 20684,\n",
              "   'start': 207.48,\n",
              "   'end': 212.6,\n",
              "   'text': \" So we're going to work on just a slice of the data frame with the power step pinnacle insoles.\",\n",
              "   'tokens': [50396,\n",
              "    407,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    589,\n",
              "    322,\n",
              "    445,\n",
              "    257,\n",
              "    13153,\n",
              "    295,\n",
              "    264,\n",
              "    1412,\n",
              "    3920,\n",
              "    365,\n",
              "    264,\n",
              "    1347,\n",
              "    1823,\n",
              "    5447,\n",
              "    77,\n",
              "    7041,\n",
              "    1028,\n",
              "    7456,\n",
              "    13,\n",
              "    50652],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12354627939370963,\n",
              "   'compression_ratio': 1.8357487922705313,\n",
              "   'no_speech_prob': 0.006379325408488512},\n",
              "  {'id': 37,\n",
              "   'seek': 20684,\n",
              "   'start': 214.6,\n",
              "   'end': 219.8,\n",
              "   'text': \" And as for embeddings, I'm going to use hucking phase embeddings. And this is just to show that\",\n",
              "   'tokens': [50752,\n",
              "    400,\n",
              "    382,\n",
              "    337,\n",
              "    12240,\n",
              "    29432,\n",
              "    11,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    764,\n",
              "    276,\n",
              "    33260,\n",
              "    5574,\n",
              "    12240,\n",
              "    29432,\n",
              "    13,\n",
              "    400,\n",
              "    341,\n",
              "    307,\n",
              "    445,\n",
              "    281,\n",
              "    855,\n",
              "    300,\n",
              "    51012],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12354627939370963,\n",
              "   'compression_ratio': 1.8357487922705313,\n",
              "   'no_speech_prob': 0.006379325408488512},\n",
              "  {'id': 38,\n",
              "   'seek': 20684,\n",
              "   'start': 219.8,\n",
              "   'end': 226.92000000000002,\n",
              "   'text': \" we don't have to use openAI for this. I'm going to create a new column in the data frame with the\",\n",
              "   'tokens': [51012,\n",
              "    321,\n",
              "    500,\n",
              "    380,\n",
              "    362,\n",
              "    281,\n",
              "    764,\n",
              "    1269,\n",
              "    48698,\n",
              "    337,\n",
              "    341,\n",
              "    13,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    1884,\n",
              "    257,\n",
              "    777,\n",
              "    7738,\n",
              "    294,\n",
              "    264,\n",
              "    1412,\n",
              "    3920,\n",
              "    365,\n",
              "    264,\n",
              "    51368],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12354627939370963,\n",
              "   'compression_ratio': 1.8357487922705313,\n",
              "   'no_speech_prob': 0.006379325408488512},\n",
              "  {'id': 39,\n",
              "   'seek': 20684,\n",
              "   'start': 226.92000000000002,\n",
              "   'end': 232.44,\n",
              "   'text': \" embedding vectors. So I'm going to use the apply function on the data frame and the word of\",\n",
              "   'tokens': [51368,\n",
              "    12240,\n",
              "    3584,\n",
              "    18875,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    764,\n",
              "    264,\n",
              "    3079,\n",
              "    2445,\n",
              "    322,\n",
              "    264,\n",
              "    1412,\n",
              "    3920,\n",
              "    293,\n",
              "    264,\n",
              "    1349,\n",
              "    295,\n",
              "    51644],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12354627939370963,\n",
              "   'compression_ratio': 1.8357487922705313,\n",
              "   'no_speech_prob': 0.006379325408488512},\n",
              "  {'id': 40,\n",
              "   'seek': 23244,\n",
              "   'start': 232.44,\n",
              "   'end': 239.4,\n",
              "   'text': \" caution here. Another reason I'm not using the openAI embeddings is because of the openAI pricing\",\n",
              "   'tokens': [50364,\n",
              "    23585,\n",
              "    510,\n",
              "    13,\n",
              "    3996,\n",
              "    1778,\n",
              "    286,\n",
              "    478,\n",
              "    406,\n",
              "    1228,\n",
              "    264,\n",
              "    1269,\n",
              "    48698,\n",
              "    12240,\n",
              "    29432,\n",
              "    307,\n",
              "    570,\n",
              "    295,\n",
              "    264,\n",
              "    1269,\n",
              "    48698,\n",
              "    17621,\n",
              "    50712],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08149946841996969,\n",
              "   'compression_ratio': 1.7008547008547008,\n",
              "   'no_speech_prob': 0.002979367971420288},\n",
              "  {'id': 41,\n",
              "   'seek': 23244,\n",
              "   'start': 239.4,\n",
              "   'end': 245.72,\n",
              "   'text': \" model. When you're running apply with an embedding function on a large data frame, you risk incurring\",\n",
              "   'tokens': [50712,\n",
              "    2316,\n",
              "    13,\n",
              "    1133,\n",
              "    291,\n",
              "    434,\n",
              "    2614,\n",
              "    3079,\n",
              "    365,\n",
              "    364,\n",
              "    12240,\n",
              "    3584,\n",
              "    2445,\n",
              "    322,\n",
              "    257,\n",
              "    2416,\n",
              "    1412,\n",
              "    3920,\n",
              "    11,\n",
              "    291,\n",
              "    3148,\n",
              "    35774,\n",
              "    2937,\n",
              "    51028],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08149946841996969,\n",
              "   'compression_ratio': 1.7008547008547008,\n",
              "   'no_speech_prob': 0.002979367971420288},\n",
              "  {'id': 42,\n",
              "   'seek': 23244,\n",
              "   'start': 245.72,\n",
              "   'end': 252.68,\n",
              "   'text': \" significant costs using openAI. Here we're only working on a slice of the original full data frame,\",\n",
              "   'tokens': [51028,\n",
              "    4776,\n",
              "    5497,\n",
              "    1228,\n",
              "    1269,\n",
              "    48698,\n",
              "    13,\n",
              "    1692,\n",
              "    321,\n",
              "    434,\n",
              "    787,\n",
              "    1364,\n",
              "    322,\n",
              "    257,\n",
              "    13153,\n",
              "    295,\n",
              "    264,\n",
              "    3380,\n",
              "    1577,\n",
              "    1412,\n",
              "    3920,\n",
              "    11,\n",
              "    51376],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08149946841996969,\n",
              "   'compression_ratio': 1.7008547008547008,\n",
              "   'no_speech_prob': 0.002979367971420288},\n",
              "  {'id': 43,\n",
              "   'seek': 23244,\n",
              "   'start': 252.68,\n",
              "   'end': 259.48,\n",
              "   'text': \" but the full data frame has more than 800,000 rows. So please don't run the apply function with an\",\n",
              "   'tokens': [51376,\n",
              "    457,\n",
              "    264,\n",
              "    1577,\n",
              "    1412,\n",
              "    3920,\n",
              "    575,\n",
              "    544,\n",
              "    813,\n",
              "    13083,\n",
              "    11,\n",
              "    1360,\n",
              "    13241,\n",
              "    13,\n",
              "    407,\n",
              "    1767,\n",
              "    500,\n",
              "    380,\n",
              "    1190,\n",
              "    264,\n",
              "    3079,\n",
              "    2445,\n",
              "    365,\n",
              "    364,\n",
              "    51716],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08149946841996969,\n",
              "   'compression_ratio': 1.7008547008547008,\n",
              "   'no_speech_prob': 0.002979367971420288},\n",
              "  {'id': 44,\n",
              "   'seek': 25948,\n",
              "   'start': 259.56,\n",
              "   'end': 265.24,\n",
              "   'text': \" openAI embedding unless you know what you're doing. All right, here we have the embedding vectors\",\n",
              "   'tokens': [50368,\n",
              "    1269,\n",
              "    48698,\n",
              "    12240,\n",
              "    3584,\n",
              "    5969,\n",
              "    291,\n",
              "    458,\n",
              "    437,\n",
              "    291,\n",
              "    434,\n",
              "    884,\n",
              "    13,\n",
              "    1057,\n",
              "    558,\n",
              "    11,\n",
              "    510,\n",
              "    321,\n",
              "    362,\n",
              "    264,\n",
              "    12240,\n",
              "    3584,\n",
              "    18875,\n",
              "    50652],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07242399646389869,\n",
              "   'compression_ratio': 1.8669201520912548,\n",
              "   'no_speech_prob': 0.00271397246979177},\n",
              "  {'id': 45,\n",
              "   'seek': 25948,\n",
              "   'start': 265.24,\n",
              "   'end': 270.36,\n",
              "   'text': \" as a new column in the data frame. And what I'm going to do now is I'm going to show you the\",\n",
              "   'tokens': [50652,\n",
              "    382,\n",
              "    257,\n",
              "    777,\n",
              "    7738,\n",
              "    294,\n",
              "    264,\n",
              "    1412,\n",
              "    3920,\n",
              "    13,\n",
              "    400,\n",
              "    437,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    360,\n",
              "    586,\n",
              "    307,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    855,\n",
              "    291,\n",
              "    264,\n",
              "    50908],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07242399646389869,\n",
              "   'compression_ratio': 1.8669201520912548,\n",
              "   'no_speech_prob': 0.00271397246979177},\n",
              "  {'id': 46,\n",
              "   'seek': 25948,\n",
              "   'start': 270.36,\n",
              "   'end': 276.76,\n",
              "   'text': \" richness of this review data. And I'll do that by training a simple random forest machine learning\",\n",
              "   'tokens': [50908,\n",
              "    44506,\n",
              "    295,\n",
              "    341,\n",
              "    3131,\n",
              "    1412,\n",
              "    13,\n",
              "    400,\n",
              "    286,\n",
              "    603,\n",
              "    360,\n",
              "    300,\n",
              "    538,\n",
              "    3097,\n",
              "    257,\n",
              "    2199,\n",
              "    4974,\n",
              "    6719,\n",
              "    3479,\n",
              "    2539,\n",
              "    51228],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07242399646389869,\n",
              "   'compression_ratio': 1.8669201520912548,\n",
              "   'no_speech_prob': 0.00271397246979177},\n",
              "  {'id': 47,\n",
              "   'seek': 25948,\n",
              "   'start': 276.76,\n",
              "   'end': 283.0,\n",
              "   'text': \" model with the embedding vectors as features and the overall rating as a target. I'll use scikit-learn\",\n",
              "   'tokens': [51228,\n",
              "    2316,\n",
              "    365,\n",
              "    264,\n",
              "    12240,\n",
              "    3584,\n",
              "    18875,\n",
              "    382,\n",
              "    4122,\n",
              "    293,\n",
              "    264,\n",
              "    4787,\n",
              "    10990,\n",
              "    382,\n",
              "    257,\n",
              "    3779,\n",
              "    13,\n",
              "    286,\n",
              "    603,\n",
              "    764,\n",
              "    2180,\n",
              "    22681,\n",
              "    12,\n",
              "    306,\n",
              "    1083,\n",
              "    51540],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07242399646389869,\n",
              "   'compression_ratio': 1.8669201520912548,\n",
              "   'no_speech_prob': 0.00271397246979177},\n",
              "  {'id': 48,\n",
              "   'seek': 25948,\n",
              "   'start': 283.0,\n",
              "   'end': 288.20000000000005,\n",
              "   'text': \" to divide the data into a training sample and a test sample. And then I'm going to import a random\",\n",
              "   'tokens': [51540,\n",
              "    281,\n",
              "    9845,\n",
              "    264,\n",
              "    1412,\n",
              "    666,\n",
              "    257,\n",
              "    3097,\n",
              "    6889,\n",
              "    293,\n",
              "    257,\n",
              "    1500,\n",
              "    6889,\n",
              "    13,\n",
              "    400,\n",
              "    550,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    974,\n",
              "    257,\n",
              "    4974,\n",
              "    51800],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07242399646389869,\n",
              "   'compression_ratio': 1.8669201520912548,\n",
              "   'no_speech_prob': 0.00271397246979177},\n",
              "  {'id': 49,\n",
              "   'seek': 28820,\n",
              "   'start': 288.2,\n",
              "   'end': 294.12,\n",
              "   'text': \" forest regressor. So I'm treating this as a regression problem, meaning that the prediction is\",\n",
              "   'tokens': [50364,\n",
              "    6719,\n",
              "    1121,\n",
              "    735,\n",
              "    284,\n",
              "    13,\n",
              "    407,\n",
              "    286,\n",
              "    478,\n",
              "    15083,\n",
              "    341,\n",
              "    382,\n",
              "    257,\n",
              "    24590,\n",
              "    1154,\n",
              "    11,\n",
              "    3620,\n",
              "    300,\n",
              "    264,\n",
              "    17630,\n",
              "    307,\n",
              "    50660],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07844590332548497,\n",
              "   'compression_ratio': 1.7455197132616487,\n",
              "   'no_speech_prob': 0.002713889814913273},\n",
              "  {'id': 50,\n",
              "   'seek': 28820,\n",
              "   'start': 294.12,\n",
              "   'end': 299.88,\n",
              "   'text': ' on a continuum, even though that we know that the rating is an integer. But this is fine for',\n",
              "   'tokens': [50660,\n",
              "    322,\n",
              "    257,\n",
              "    36120,\n",
              "    11,\n",
              "    754,\n",
              "    1673,\n",
              "    300,\n",
              "    321,\n",
              "    458,\n",
              "    300,\n",
              "    264,\n",
              "    10990,\n",
              "    307,\n",
              "    364,\n",
              "    24922,\n",
              "    13,\n",
              "    583,\n",
              "    341,\n",
              "    307,\n",
              "    2489,\n",
              "    337,\n",
              "    50948],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07844590332548497,\n",
              "   'compression_ratio': 1.7455197132616487,\n",
              "   'no_speech_prob': 0.002713889814913273},\n",
              "  {'id': 51,\n",
              "   'seek': 28820,\n",
              "   'start': 299.88,\n",
              "   'end': 305.0,\n",
              "   'text': \" demonstration. And then I'm training the model on the training part of the data set. And here you\",\n",
              "   'tokens': [50948,\n",
              "    16520,\n",
              "    13,\n",
              "    400,\n",
              "    550,\n",
              "    286,\n",
              "    478,\n",
              "    3097,\n",
              "    264,\n",
              "    2316,\n",
              "    322,\n",
              "    264,\n",
              "    3097,\n",
              "    644,\n",
              "    295,\n",
              "    264,\n",
              "    1412,\n",
              "    992,\n",
              "    13,\n",
              "    400,\n",
              "    510,\n",
              "    291,\n",
              "    51204],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07844590332548497,\n",
              "   'compression_ratio': 1.7455197132616487,\n",
              "   'no_speech_prob': 0.002713889814913273},\n",
              "  {'id': 52,\n",
              "   'seek': 28820,\n",
              "   'start': 305.0,\n",
              "   'end': 310.76,\n",
              "   'text': \" can also play around with the number of estimators you're using. I found that 150 was fine to make this\",\n",
              "   'tokens': [51204,\n",
              "    393,\n",
              "    611,\n",
              "    862,\n",
              "    926,\n",
              "    365,\n",
              "    264,\n",
              "    1230,\n",
              "    295,\n",
              "    8017,\n",
              "    3391,\n",
              "    291,\n",
              "    434,\n",
              "    1228,\n",
              "    13,\n",
              "    286,\n",
              "    1352,\n",
              "    300,\n",
              "    8451,\n",
              "    390,\n",
              "    2489,\n",
              "    281,\n",
              "    652,\n",
              "    341,\n",
              "    51492],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07844590332548497,\n",
              "   'compression_ratio': 1.7455197132616487,\n",
              "   'no_speech_prob': 0.002713889814913273},\n",
              "  {'id': 53,\n",
              "   'seek': 28820,\n",
              "   'start': 310.76,\n",
              "   'end': 317.0,\n",
              "   'text': ' point. And once the fitting is done, we can evaluate by using the mean absolute error on the test',\n",
              "   'tokens': [51492,\n",
              "    935,\n",
              "    13,\n",
              "    400,\n",
              "    1564,\n",
              "    264,\n",
              "    15669,\n",
              "    307,\n",
              "    1096,\n",
              "    11,\n",
              "    321,\n",
              "    393,\n",
              "    13059,\n",
              "    538,\n",
              "    1228,\n",
              "    264,\n",
              "    914,\n",
              "    8236,\n",
              "    6713,\n",
              "    322,\n",
              "    264,\n",
              "    1500,\n",
              "    51804],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.07844590332548497,\n",
              "   'compression_ratio': 1.7455197132616487,\n",
              "   'no_speech_prob': 0.002713889814913273},\n",
              "  {'id': 54,\n",
              "   'seek': 31700,\n",
              "   'start': 317.0,\n",
              "   'end': 324.04,\n",
              "   'text': ' part of the data set. And we see that we achieve a mean absolute error of 0.53, which means that',\n",
              "   'tokens': [50364,\n",
              "    644,\n",
              "    295,\n",
              "    264,\n",
              "    1412,\n",
              "    992,\n",
              "    13,\n",
              "    400,\n",
              "    321,\n",
              "    536,\n",
              "    300,\n",
              "    321,\n",
              "    4584,\n",
              "    257,\n",
              "    914,\n",
              "    8236,\n",
              "    6713,\n",
              "    295,\n",
              "    1958,\n",
              "    13,\n",
              "    19584,\n",
              "    11,\n",
              "    597,\n",
              "    1355,\n",
              "    300,\n",
              "    50716],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10036854553222656,\n",
              "   'compression_ratio': 1.5934426229508196,\n",
              "   'no_speech_prob': 0.0038234142120927572},\n",
              "  {'id': 55,\n",
              "   'seek': 31700,\n",
              "   'start': 324.04,\n",
              "   'end': 330.28,\n",
              "   'text': ' on average, the prediction is off by around half a point. And this is with a simple non-optimized',\n",
              "   'tokens': [50716,\n",
              "    322,\n",
              "    4274,\n",
              "    11,\n",
              "    264,\n",
              "    17630,\n",
              "    307,\n",
              "    766,\n",
              "    538,\n",
              "    926,\n",
              "    1922,\n",
              "    257,\n",
              "    935,\n",
              "    13,\n",
              "    400,\n",
              "    341,\n",
              "    307,\n",
              "    365,\n",
              "    257,\n",
              "    2199,\n",
              "    2107,\n",
              "    12,\n",
              "    5747,\n",
              "    332,\n",
              "    1602,\n",
              "    51028],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10036854553222656,\n",
              "   'compression_ratio': 1.5934426229508196,\n",
              "   'no_speech_prob': 0.0038234142120927572},\n",
              "  {'id': 56,\n",
              "   'seek': 31700,\n",
              "   'start': 330.28,\n",
              "   'end': 335.08,\n",
              "   'text': ' machine learning model that takes five minutes to run. If you spend some time optimizing this,',\n",
              "   'tokens': [51028,\n",
              "    3479,\n",
              "    2539,\n",
              "    2316,\n",
              "    300,\n",
              "    2516,\n",
              "    1732,\n",
              "    2077,\n",
              "    281,\n",
              "    1190,\n",
              "    13,\n",
              "    759,\n",
              "    291,\n",
              "    3496,\n",
              "    512,\n",
              "    565,\n",
              "    40425,\n",
              "    341,\n",
              "    11,\n",
              "    51268],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10036854553222656,\n",
              "   'compression_ratio': 1.5934426229508196,\n",
              "   'no_speech_prob': 0.0038234142120927572},\n",
              "  {'id': 57,\n",
              "   'seek': 31700,\n",
              "   'start': 335.08,\n",
              "   'end': 340.2,\n",
              "   'text': ' are building a more advanced model in PyTorch, you could probably get to 0.3 or below. So there',\n",
              "   'tokens': [51268,\n",
              "    366,\n",
              "    2390,\n",
              "    257,\n",
              "    544,\n",
              "    7339,\n",
              "    2316,\n",
              "    294,\n",
              "    9953,\n",
              "    51,\n",
              "    284,\n",
              "    339,\n",
              "    11,\n",
              "    291,\n",
              "    727,\n",
              "    1391,\n",
              "    483,\n",
              "    281,\n",
              "    1958,\n",
              "    13,\n",
              "    18,\n",
              "    420,\n",
              "    2507,\n",
              "    13,\n",
              "    407,\n",
              "    456,\n",
              "    51524],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10036854553222656,\n",
              "   'compression_ratio': 1.5934426229508196,\n",
              "   'no_speech_prob': 0.0038234142120927572},\n",
              "  {'id': 58,\n",
              "   'seek': 31700,\n",
              "   'start': 340.2,\n",
              "   'end': 345.56,\n",
              "   'text': \" is significant signal in this data. But why would you want to predict the rating? Well, you wouldn't\",\n",
              "   'tokens': [51524,\n",
              "    307,\n",
              "    4776,\n",
              "    6358,\n",
              "    294,\n",
              "    341,\n",
              "    1412,\n",
              "    13,\n",
              "    583,\n",
              "    983,\n",
              "    576,\n",
              "    291,\n",
              "    528,\n",
              "    281,\n",
              "    6069,\n",
              "    264,\n",
              "    10990,\n",
              "    30,\n",
              "    1042,\n",
              "    11,\n",
              "    291,\n",
              "    2759,\n",
              "    380,\n",
              "    51792],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10036854553222656,\n",
              "   'compression_ratio': 1.5934426229508196,\n",
              "   'no_speech_prob': 0.0038234142120927572},\n",
              "  {'id': 59,\n",
              "   'seek': 34556,\n",
              "   'start': 345.64,\n",
              "   'end': 350.2,\n",
              "   'text': \" unless you're missing the rating for some of the reviews. What you want to do is to switch out\",\n",
              "   'tokens': [50368,\n",
              "    5969,\n",
              "    291,\n",
              "    434,\n",
              "    5361,\n",
              "    264,\n",
              "    10990,\n",
              "    337,\n",
              "    512,\n",
              "    295,\n",
              "    264,\n",
              "    10229,\n",
              "    13,\n",
              "    708,\n",
              "    291,\n",
              "    528,\n",
              "    281,\n",
              "    360,\n",
              "    307,\n",
              "    281,\n",
              "    3679,\n",
              "    484,\n",
              "    50596],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08121964783795112,\n",
              "   'compression_ratio': 1.7132867132867133,\n",
              "   'no_speech_prob': 0.0017530762124806643},\n",
              "  {'id': 60,\n",
              "   'seek': 34556,\n",
              "   'start': 350.2,\n",
              "   'end': 356.44,\n",
              "   'text': ' the target variable and use the signal in the review data to build machine learning models that',\n",
              "   'tokens': [50596,\n",
              "    264,\n",
              "    3779,\n",
              "    7006,\n",
              "    293,\n",
              "    764,\n",
              "    264,\n",
              "    6358,\n",
              "    294,\n",
              "    264,\n",
              "    3131,\n",
              "    1412,\n",
              "    281,\n",
              "    1322,\n",
              "    3479,\n",
              "    2539,\n",
              "    5245,\n",
              "    300,\n",
              "    50908],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08121964783795112,\n",
              "   'compression_ratio': 1.7132867132867133,\n",
              "   'no_speech_prob': 0.0017530762124806643},\n",
              "  {'id': 61,\n",
              "   'seek': 34556,\n",
              "   'start': 356.44,\n",
              "   'end': 362.76,\n",
              "   'text': ' will actually help you generate value. And these are product recommendation models, churn or retention',\n",
              "   'tokens': [50908,\n",
              "    486,\n",
              "    767,\n",
              "    854,\n",
              "    291,\n",
              "    8460,\n",
              "    2158,\n",
              "    13,\n",
              "    400,\n",
              "    613,\n",
              "    366,\n",
              "    1674,\n",
              "    11879,\n",
              "    5245,\n",
              "    11,\n",
              "    417,\n",
              "    925,\n",
              "    420,\n",
              "    22871,\n",
              "    51224],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08121964783795112,\n",
              "   'compression_ratio': 1.7132867132867133,\n",
              "   'no_speech_prob': 0.0017530762124806643},\n",
              "  {'id': 62,\n",
              "   'seek': 34556,\n",
              "   'start': 362.76,\n",
              "   'end': 369.56,\n",
              "   'text': \" models, propensity models, uplift models, and so on. I'm now going to load the review embeddings\",\n",
              "   'tokens': [51224,\n",
              "    5245,\n",
              "    11,\n",
              "    2365,\n",
              "    6859,\n",
              "    5245,\n",
              "    11,\n",
              "    45407,\n",
              "    5245,\n",
              "    11,\n",
              "    293,\n",
              "    370,\n",
              "    322,\n",
              "    13,\n",
              "    286,\n",
              "    478,\n",
              "    586,\n",
              "    516,\n",
              "    281,\n",
              "    3677,\n",
              "    264,\n",
              "    3131,\n",
              "    12240,\n",
              "    29432,\n",
              "    51564],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08121964783795112,\n",
              "   'compression_ratio': 1.7132867132867133,\n",
              "   'no_speech_prob': 0.0017530762124806643},\n",
              "  {'id': 63,\n",
              "   'seek': 34556,\n",
              "   'start': 369.56,\n",
              "   'end': 375.16,\n",
              "   'text': ' into the vector database and show you how we can have GBT for access the data and give us a summary',\n",
              "   'tokens': [51564,\n",
              "    666,\n",
              "    264,\n",
              "    8062,\n",
              "    8149,\n",
              "    293,\n",
              "    855,\n",
              "    291,\n",
              "    577,\n",
              "    321,\n",
              "    393,\n",
              "    362,\n",
              "    460,\n",
              "    33853,\n",
              "    337,\n",
              "    2105,\n",
              "    264,\n",
              "    1412,\n",
              "    293,\n",
              "    976,\n",
              "    505,\n",
              "    257,\n",
              "    12691,\n",
              "    51844],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08121964783795112,\n",
              "   'compression_ratio': 1.7132867132867133,\n",
              "   'no_speech_prob': 0.0017530762124806643},\n",
              "  {'id': 64,\n",
              "   'seek': 37516,\n",
              "   'start': 375.16,\n",
              "   'end': 380.28000000000003,\n",
              "   'text': \" of the reviews. I'm using GBT for as it's currently the most powerful language model we have to\",\n",
              "   'tokens': [50364,\n",
              "    295,\n",
              "    264,\n",
              "    10229,\n",
              "    13,\n",
              "    286,\n",
              "    478,\n",
              "    1228,\n",
              "    460,\n",
              "    33853,\n",
              "    337,\n",
              "    382,\n",
              "    309,\n",
              "    311,\n",
              "    4362,\n",
              "    264,\n",
              "    881,\n",
              "    4005,\n",
              "    2856,\n",
              "    2316,\n",
              "    321,\n",
              "    362,\n",
              "    281,\n",
              "    50620],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.172998762622322,\n",
              "   'compression_ratio': 1.6835443037974684,\n",
              "   'no_speech_prob': 0.00020649502403102815},\n",
              "  {'id': 65,\n",
              "   'seek': 37516,\n",
              "   'start': 380.28000000000003,\n",
              "   'end': 386.6,\n",
              "   'text': ' work with, but you can switch out the language model as you see fit. To upload the review embeddings,',\n",
              "   'tokens': [50620,\n",
              "    589,\n",
              "    365,\n",
              "    11,\n",
              "    457,\n",
              "    291,\n",
              "    393,\n",
              "    3679,\n",
              "    484,\n",
              "    264,\n",
              "    2856,\n",
              "    2316,\n",
              "    382,\n",
              "    291,\n",
              "    536,\n",
              "    3318,\n",
              "    13,\n",
              "    1407,\n",
              "    6580,\n",
              "    264,\n",
              "    3131,\n",
              "    12240,\n",
              "    29432,\n",
              "    11,\n",
              "    50936],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.172998762622322,\n",
              "   'compression_ratio': 1.6835443037974684,\n",
              "   'no_speech_prob': 0.00020649502403102815},\n",
              "  {'id': 66,\n",
              "   'seek': 37516,\n",
              "   'start': 386.6,\n",
              "   'end': 392.68,\n",
              "   'text': ' we import and initiate pine cone and then transform the truncated review text column into a list of reviews.',\n",
              "   'tokens': [50936,\n",
              "    321,\n",
              "    974,\n",
              "    293,\n",
              "    31574,\n",
              "    15113,\n",
              "    19749,\n",
              "    293,\n",
              "    550,\n",
              "    4088,\n",
              "    264,\n",
              "    504,\n",
              "    409,\n",
              "    66,\n",
              "    770,\n",
              "    3131,\n",
              "    2487,\n",
              "    7738,\n",
              "    666,\n",
              "    257,\n",
              "    1329,\n",
              "    295,\n",
              "    10229,\n",
              "    13,\n",
              "    51240],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.172998762622322,\n",
              "   'compression_ratio': 1.6835443037974684,\n",
              "   'no_speech_prob': 0.00020649502403102815},\n",
              "  {'id': 67,\n",
              "   'seek': 37516,\n",
              "   'start': 398.28000000000003,\n",
              "   'end': 403.64000000000004,\n",
              "   'text': ' Then we upload the reviews with the built-in from text method using Hocking Face embeddings.',\n",
              "   'tokens': [51520,\n",
              "    1396,\n",
              "    321,\n",
              "    6580,\n",
              "    264,\n",
              "    10229,\n",
              "    365,\n",
              "    264,\n",
              "    3094,\n",
              "    12,\n",
              "    259,\n",
              "    490,\n",
              "    2487,\n",
              "    3170,\n",
              "    1228,\n",
              "    389,\n",
              "    31730,\n",
              "    4047,\n",
              "    12240,\n",
              "    29432,\n",
              "    13,\n",
              "    51788],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.172998762622322,\n",
              "   'compression_ratio': 1.6835443037974684,\n",
              "   'no_speech_prob': 0.00020649502403102815},\n",
              "  {'id': 68,\n",
              "   'seek': 40516,\n",
              "   'start': 405.16,\n",
              "   'end': 409.48,\n",
              "   'text': ' Once the upload is done, we can head over to pine cone and check that the vectors have been uploaded.',\n",
              "   'tokens': [50364,\n",
              "    3443,\n",
              "    264,\n",
              "    6580,\n",
              "    307,\n",
              "    1096,\n",
              "    11,\n",
              "    321,\n",
              "    393,\n",
              "    1378,\n",
              "    670,\n",
              "    281,\n",
              "    15113,\n",
              "    19749,\n",
              "    293,\n",
              "    1520,\n",
              "    300,\n",
              "    264,\n",
              "    18875,\n",
              "    362,\n",
              "    668,\n",
              "    17135,\n",
              "    13,\n",
              "    50580],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10058328329798687,\n",
              "   'compression_ratio': 1.7563451776649746,\n",
              "   'no_speech_prob': 0.0002332972944714129},\n",
              "  {'id': 69,\n",
              "   'seek': 40516,\n",
              "   'start': 412.92,\n",
              "   'end': 417.32000000000005,\n",
              "   'text': ' And here we have all the review vectors uploaded into pine cone.',\n",
              "   'tokens': [50752,\n",
              "    400,\n",
              "    510,\n",
              "    321,\n",
              "    362,\n",
              "    439,\n",
              "    264,\n",
              "    3131,\n",
              "    18875,\n",
              "    17135,\n",
              "    666,\n",
              "    15113,\n",
              "    19749,\n",
              "    13,\n",
              "    50972],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10058328329798687,\n",
              "   'compression_ratio': 1.7563451776649746,\n",
              "   'no_speech_prob': 0.0002332972944714129},\n",
              "  {'id': 70,\n",
              "   'seek': 40516,\n",
              "   'start': 420.68,\n",
              "   'end': 424.6,\n",
              "   'text': ' And then we can do a basic vector similarity search to check that everything works.',\n",
              "   'tokens': [51140,\n",
              "    400,\n",
              "    550,\n",
              "    321,\n",
              "    393,\n",
              "    360,\n",
              "    257,\n",
              "    3875,\n",
              "    8062,\n",
              "    32194,\n",
              "    3164,\n",
              "    281,\n",
              "    1520,\n",
              "    300,\n",
              "    1203,\n",
              "    1985,\n",
              "    13,\n",
              "    51336],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10058328329798687,\n",
              "   'compression_ratio': 1.7563451776649746,\n",
              "   'no_speech_prob': 0.0002332972944714129},\n",
              "  {'id': 71,\n",
              "   'seek': 40516,\n",
              "   'start': 426.76000000000005,\n",
              "   'end': 430.68,\n",
              "   'text': ' What I want to do now is I want to have the language model access the data in the vector store.',\n",
              "   'tokens': [51444,\n",
              "    708,\n",
              "    286,\n",
              "    528,\n",
              "    281,\n",
              "    360,\n",
              "    586,\n",
              "    307,\n",
              "    286,\n",
              "    528,\n",
              "    281,\n",
              "    362,\n",
              "    264,\n",
              "    2856,\n",
              "    2316,\n",
              "    2105,\n",
              "    264,\n",
              "    1412,\n",
              "    294,\n",
              "    264,\n",
              "    8062,\n",
              "    3531,\n",
              "    13,\n",
              "    51640],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.10058328329798687,\n",
              "   'compression_ratio': 1.7563451776649746,\n",
              "   'no_speech_prob': 0.0002332972944714129},\n",
              "  {'id': 72,\n",
              "   'seek': 43068,\n",
              "   'start': 430.68,\n",
              "   'end': 435.88,\n",
              "   'text': \" So I'm going to import retrieval QA that is used to retrieve the most relevant reviews given\",\n",
              "   'tokens': [50364,\n",
              "    407,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    974,\n",
              "    19817,\n",
              "    3337,\n",
              "    1249,\n",
              "    32,\n",
              "    300,\n",
              "    307,\n",
              "    1143,\n",
              "    281,\n",
              "    30254,\n",
              "    264,\n",
              "    881,\n",
              "    7340,\n",
              "    10229,\n",
              "    2212,\n",
              "    50624],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1670921288647698,\n",
              "   'compression_ratio': 1.7844036697247707,\n",
              "   'no_speech_prob': 0.008431055583059788},\n",
              "  {'id': 73,\n",
              "   'seek': 43068,\n",
              "   'start': 435.88,\n",
              "   'end': 441.08,\n",
              "   'text': \" a prompt and feed those to the language model. And then I'm going to import chat.obmAPI that I'm\",\n",
              "   'tokens': [50624,\n",
              "    257,\n",
              "    12391,\n",
              "    293,\n",
              "    3154,\n",
              "    729,\n",
              "    281,\n",
              "    264,\n",
              "    2856,\n",
              "    2316,\n",
              "    13,\n",
              "    400,\n",
              "    550,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    974,\n",
              "    5081,\n",
              "    13,\n",
              "    996,\n",
              "    76,\n",
              "    4715,\n",
              "    40,\n",
              "    300,\n",
              "    286,\n",
              "    478,\n",
              "    50884],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1670921288647698,\n",
              "   'compression_ratio': 1.7844036697247707,\n",
              "   'no_speech_prob': 0.008431055583059788},\n",
              "  {'id': 74,\n",
              "   'seek': 43068,\n",
              "   'start': 441.08,\n",
              "   'end': 448.84000000000003,\n",
              "   'text': \" going to use as a driver around GBT for. Then we're going to define a chain. It's called review chain\",\n",
              "   'tokens': [50884,\n",
              "    516,\n",
              "    281,\n",
              "    764,\n",
              "    382,\n",
              "    257,\n",
              "    6787,\n",
              "    926,\n",
              "    460,\n",
              "    33853,\n",
              "    337,\n",
              "    13,\n",
              "    1396,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    6964,\n",
              "    257,\n",
              "    5021,\n",
              "    13,\n",
              "    467,\n",
              "    311,\n",
              "    1219,\n",
              "    3131,\n",
              "    5021,\n",
              "    51272],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1670921288647698,\n",
              "   'compression_ratio': 1.7844036697247707,\n",
              "   'no_speech_prob': 0.008431055583059788},\n",
              "  {'id': 75,\n",
              "   'seek': 43068,\n",
              "   'start': 448.84000000000003,\n",
              "   'end': 455.24,\n",
              "   'text': ' using retrieval QA that takes the language model, the vector store, and then the chain type as an',\n",
              "   'tokens': [51272,\n",
              "    1228,\n",
              "    19817,\n",
              "    3337,\n",
              "    1249,\n",
              "    32,\n",
              "    300,\n",
              "    2516,\n",
              "    264,\n",
              "    2856,\n",
              "    2316,\n",
              "    11,\n",
              "    264,\n",
              "    8062,\n",
              "    3531,\n",
              "    11,\n",
              "    293,\n",
              "    550,\n",
              "    264,\n",
              "    5021,\n",
              "    2010,\n",
              "    382,\n",
              "    364,\n",
              "    51592],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1670921288647698,\n",
              "   'compression_ratio': 1.7844036697247707,\n",
              "   'no_speech_prob': 0.008431055583059788},\n",
              "  {'id': 76,\n",
              "   'seek': 45524,\n",
              "   'start': 455.24,\n",
              "   'end': 460.28000000000003,\n",
              "   'text': ' argument. And here we use chain type stuff, which means that we stuff all the related data into',\n",
              "   'tokens': [50364,\n",
              "    6770,\n",
              "    13,\n",
              "    400,\n",
              "    510,\n",
              "    321,\n",
              "    764,\n",
              "    5021,\n",
              "    2010,\n",
              "    1507,\n",
              "    11,\n",
              "    597,\n",
              "    1355,\n",
              "    300,\n",
              "    321,\n",
              "    1507,\n",
              "    439,\n",
              "    264,\n",
              "    4077,\n",
              "    1412,\n",
              "    666,\n",
              "    50616],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1356257525357333,\n",
              "   'compression_ratio': 1.768060836501901,\n",
              "   'no_speech_prob': 0.003705016104504466},\n",
              "  {'id': 77,\n",
              "   'seek': 45524,\n",
              "   'start': 460.28000000000003,\n",
              "   'end': 465.88,\n",
              "   'text': ' the prompt and we use that as a context and pass it to the language model. Then we simply write',\n",
              "   'tokens': [50616,\n",
              "    264,\n",
              "    12391,\n",
              "    293,\n",
              "    321,\n",
              "    764,\n",
              "    300,\n",
              "    382,\n",
              "    257,\n",
              "    4319,\n",
              "    293,\n",
              "    1320,\n",
              "    309,\n",
              "    281,\n",
              "    264,\n",
              "    2856,\n",
              "    2316,\n",
              "    13,\n",
              "    1396,\n",
              "    321,\n",
              "    2935,\n",
              "    2464,\n",
              "    50896],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1356257525357333,\n",
              "   'compression_ratio': 1.768060836501901,\n",
              "   'no_speech_prob': 0.003705016104504466},\n",
              "  {'id': 78,\n",
              "   'seek': 45524,\n",
              "   'start': 465.88,\n",
              "   'end': 471.88,\n",
              "   'text': ' the query as we usually do when working with chat.obt and we run the chain with this query.',\n",
              "   'tokens': [50896,\n",
              "    264,\n",
              "    14581,\n",
              "    382,\n",
              "    321,\n",
              "    2673,\n",
              "    360,\n",
              "    562,\n",
              "    1364,\n",
              "    365,\n",
              "    5081,\n",
              "    13,\n",
              "    996,\n",
              "    83,\n",
              "    293,\n",
              "    321,\n",
              "    1190,\n",
              "    264,\n",
              "    5021,\n",
              "    365,\n",
              "    341,\n",
              "    14581,\n",
              "    13,\n",
              "    51196],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1356257525357333,\n",
              "   'compression_ratio': 1.768060836501901,\n",
              "   'no_speech_prob': 0.003705016104504466},\n",
              "  {'id': 79,\n",
              "   'seek': 45524,\n",
              "   'start': 473.16,\n",
              "   'end': 478.2,\n",
              "   'text': \" And here I'm asking to be for to give us an overall impression of the reviews and give us the\",\n",
              "   'tokens': [51260,\n",
              "    400,\n",
              "    510,\n",
              "    286,\n",
              "    478,\n",
              "    3365,\n",
              "    281,\n",
              "    312,\n",
              "    337,\n",
              "    281,\n",
              "    976,\n",
              "    505,\n",
              "    364,\n",
              "    4787,\n",
              "    9995,\n",
              "    295,\n",
              "    264,\n",
              "    10229,\n",
              "    293,\n",
              "    976,\n",
              "    505,\n",
              "    264,\n",
              "    51512],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1356257525357333,\n",
              "   'compression_ratio': 1.768060836501901,\n",
              "   'no_speech_prob': 0.003705016104504466},\n",
              "  {'id': 80,\n",
              "   'seek': 45524,\n",
              "   'start': 478.2,\n",
              "   'end': 483.40000000000003,\n",
              "   'text': ' most prevalent examples and bullet points and also give us suggestions for improvement.',\n",
              "   'tokens': [51512,\n",
              "    881,\n",
              "    30652,\n",
              "    5110,\n",
              "    293,\n",
              "    11632,\n",
              "    2793,\n",
              "    293,\n",
              "    611,\n",
              "    976,\n",
              "    505,\n",
              "    13396,\n",
              "    337,\n",
              "    10444,\n",
              "    13,\n",
              "    51772],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1356257525357333,\n",
              "   'compression_ratio': 1.768060836501901,\n",
              "   'no_speech_prob': 0.003705016104504466},\n",
              "  {'id': 81,\n",
              "   'seek': 48340,\n",
              "   'start': 484.35999999999996,\n",
              "   'end': 489.71999999999997,\n",
              "   'text': \" And remember when you're working with chat models like DBT4 you can send system messages that\",\n",
              "   'tokens': [50412,\n",
              "    400,\n",
              "    1604,\n",
              "    562,\n",
              "    291,\n",
              "    434,\n",
              "    1364,\n",
              "    365,\n",
              "    5081,\n",
              "    5245,\n",
              "    411,\n",
              "    413,\n",
              "    33853,\n",
              "    19,\n",
              "    291,\n",
              "    393,\n",
              "    2845,\n",
              "    1185,\n",
              "    7897,\n",
              "    300,\n",
              "    50680],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1404213791801816,\n",
              "   'compression_ratio': 1.6160714285714286,\n",
              "   'no_speech_prob': 0.0014057528460398316},\n",
              "  {'id': 82,\n",
              "   'seek': 48340,\n",
              "   'start': 489.71999999999997,\n",
              "   'end': 495.47999999999996,\n",
              "   'text': ' will allow you to calibrate the system of the model, which could significantly improve the',\n",
              "   'tokens': [50680,\n",
              "    486,\n",
              "    2089,\n",
              "    291,\n",
              "    281,\n",
              "    21583,\n",
              "    4404,\n",
              "    264,\n",
              "    1185,\n",
              "    295,\n",
              "    264,\n",
              "    2316,\n",
              "    11,\n",
              "    597,\n",
              "    727,\n",
              "    10591,\n",
              "    3470,\n",
              "    264,\n",
              "    50968],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1404213791801816,\n",
              "   'compression_ratio': 1.6160714285714286,\n",
              "   'no_speech_prob': 0.0014057528460398316},\n",
              "  {'id': 83,\n",
              "   'seek': 48340,\n",
              "   'start': 495.47999999999996,\n",
              "   'end': 501.64,\n",
              "   'text': \" quality of the output. Alright so here we are with DBT4's impression of the reviews.\",\n",
              "   'tokens': [50968,\n",
              "    3125,\n",
              "    295,\n",
              "    264,\n",
              "    5598,\n",
              "    13,\n",
              "    2798,\n",
              "    370,\n",
              "    510,\n",
              "    321,\n",
              "    366,\n",
              "    365,\n",
              "    413,\n",
              "    33853,\n",
              "    19,\n",
              "    311,\n",
              "    9995,\n",
              "    295,\n",
              "    264,\n",
              "    10229,\n",
              "    13,\n",
              "    51276],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1404213791801816,\n",
              "   'compression_ratio': 1.6160714285714286,\n",
              "   'no_speech_prob': 0.0014057528460398316},\n",
              "  {'id': 84,\n",
              "   'seek': 48340,\n",
              "   'start': 502.28,\n",
              "   'end': 508.28,\n",
              "   'text': ' Some examples of good reviews, mixed reviews, some negative reviews, and then based on those',\n",
              "   'tokens': [51308,\n",
              "    2188,\n",
              "    5110,\n",
              "    295,\n",
              "    665,\n",
              "    10229,\n",
              "    11,\n",
              "    7467,\n",
              "    10229,\n",
              "    11,\n",
              "    512,\n",
              "    3671,\n",
              "    10229,\n",
              "    11,\n",
              "    293,\n",
              "    550,\n",
              "    2361,\n",
              "    322,\n",
              "    729,\n",
              "    51608],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.1404213791801816,\n",
              "   'compression_ratio': 1.6160714285714286,\n",
              "   'no_speech_prob': 0.0014057528460398316},\n",
              "  {'id': 85,\n",
              "   'seek': 50828,\n",
              "   'start': 508.28,\n",
              "   'end': 515.72,\n",
              "   'text': \" reviews there are some potential areas to focus on improving. So pretty nice and I'm sure you can\",\n",
              "   'tokens': [50364,\n",
              "    10229,\n",
              "    456,\n",
              "    366,\n",
              "    512,\n",
              "    3995,\n",
              "    3179,\n",
              "    281,\n",
              "    1879,\n",
              "    322,\n",
              "    11470,\n",
              "    13,\n",
              "    407,\n",
              "    1238,\n",
              "    1481,\n",
              "    293,\n",
              "    286,\n",
              "    478,\n",
              "    988,\n",
              "    291,\n",
              "    393,\n",
              "    50736],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11410895727014028,\n",
              "   'compression_ratio': 1.6178861788617886,\n",
              "   'no_speech_prob': 0.00038485045661218464},\n",
              "  {'id': 86,\n",
              "   'seek': 50828,\n",
              "   'start': 515.72,\n",
              "   'end': 522.68,\n",
              "   'text': \" improve that by calibrating the system. What's really powerful about this is that you can turn this\",\n",
              "   'tokens': [50736,\n",
              "    3470,\n",
              "    300,\n",
              "    538,\n",
              "    2104,\n",
              "    6414,\n",
              "    990,\n",
              "    264,\n",
              "    1185,\n",
              "    13,\n",
              "    708,\n",
              "    311,\n",
              "    534,\n",
              "    4005,\n",
              "    466,\n",
              "    341,\n",
              "    307,\n",
              "    300,\n",
              "    291,\n",
              "    393,\n",
              "    1261,\n",
              "    341,\n",
              "    51084],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11410895727014028,\n",
              "   'compression_ratio': 1.6178861788617886,\n",
              "   'no_speech_prob': 0.00038485045661218464},\n",
              "  {'id': 87,\n",
              "   'seek': 50828,\n",
              "   'start': 522.68,\n",
              "   'end': 528.76,\n",
              "   'text': ' into a weekly digest and send it out to an internal email list within a company and this can also be',\n",
              "   'tokens': [51084,\n",
              "    666,\n",
              "    257,\n",
              "    12460,\n",
              "    13884,\n",
              "    293,\n",
              "    2845,\n",
              "    309,\n",
              "    484,\n",
              "    281,\n",
              "    364,\n",
              "    6920,\n",
              "    3796,\n",
              "    1329,\n",
              "    1951,\n",
              "    257,\n",
              "    2237,\n",
              "    293,\n",
              "    341,\n",
              "    393,\n",
              "    611,\n",
              "    312,\n",
              "    51388],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11410895727014028,\n",
              "   'compression_ratio': 1.6178861788617886,\n",
              "   'no_speech_prob': 0.00038485045661218464},\n",
              "  {'id': 88,\n",
              "   'seek': 50828,\n",
              "   'start': 528.76,\n",
              "   'end': 534.52,\n",
              "   'text': \" done with chain. Alright so the last thing we're going to have a look at is filtered vector similar\",\n",
              "   'tokens': [51388,\n",
              "    1096,\n",
              "    365,\n",
              "    5021,\n",
              "    13,\n",
              "    2798,\n",
              "    370,\n",
              "    264,\n",
              "    1036,\n",
              "    551,\n",
              "    321,\n",
              "    434,\n",
              "    516,\n",
              "    281,\n",
              "    362,\n",
              "    257,\n",
              "    574,\n",
              "    412,\n",
              "    307,\n",
              "    37111,\n",
              "    8062,\n",
              "    2531,\n",
              "    51676],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.11410895727014028,\n",
              "   'compression_ratio': 1.6178861788617886,\n",
              "   'no_speech_prob': 0.00038485045661218464},\n",
              "  {'id': 89,\n",
              "   'seek': 53452,\n",
              "   'start': 534.52,\n",
              "   'end': 540.84,\n",
              "   'text': \" research and what I'd like to do is to get all the reviews of a given rating that match a specific\",\n",
              "   'tokens': [50364,\n",
              "    2132,\n",
              "    293,\n",
              "    437,\n",
              "    286,\n",
              "    1116,\n",
              "    411,\n",
              "    281,\n",
              "    360,\n",
              "    307,\n",
              "    281,\n",
              "    483,\n",
              "    439,\n",
              "    264,\n",
              "    10229,\n",
              "    295,\n",
              "    257,\n",
              "    2212,\n",
              "    10990,\n",
              "    300,\n",
              "    2995,\n",
              "    257,\n",
              "    2685,\n",
              "    50680],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.133624267578125,\n",
              "   'compression_ratio': 1.6506550218340612,\n",
              "   'no_speech_prob': 0.005359705537557602},\n",
              "  {'id': 90,\n",
              "   'seek': 53452,\n",
              "   'start': 540.84,\n",
              "   'end': 547.8,\n",
              "   'text': \" theme so we need to filter on metadata to do that. I'm going to use Pinecon directly as\",\n",
              "   'tokens': [50680,\n",
              "    6314,\n",
              "    370,\n",
              "    321,\n",
              "    643,\n",
              "    281,\n",
              "    6608,\n",
              "    322,\n",
              "    26603,\n",
              "    281,\n",
              "    360,\n",
              "    300,\n",
              "    13,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    764,\n",
              "    33531,\n",
              "    1671,\n",
              "    3838,\n",
              "    382,\n",
              "    51028],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.133624267578125,\n",
              "   'compression_ratio': 1.6506550218340612,\n",
              "   'no_speech_prob': 0.005359705537557602},\n",
              "  {'id': 91,\n",
              "   'seek': 53452,\n",
              "   'start': 547.8,\n",
              "   'end': 553.8,\n",
              "   'text': \" Langchain doesn't seem to support this out of the box yet. To upload the data with the Pinecon\",\n",
              "   'tokens': [51028,\n",
              "    13313,\n",
              "    11509,\n",
              "    1177,\n",
              "    380,\n",
              "    1643,\n",
              "    281,\n",
              "    1406,\n",
              "    341,\n",
              "    484,\n",
              "    295,\n",
              "    264,\n",
              "    2424,\n",
              "    1939,\n",
              "    13,\n",
              "    1407,\n",
              "    6580,\n",
              "    264,\n",
              "    1412,\n",
              "    365,\n",
              "    264,\n",
              "    33531,\n",
              "    1671,\n",
              "    51328],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.133624267578125,\n",
              "   'compression_ratio': 1.6506550218340612,\n",
              "   'no_speech_prob': 0.005359705537557602},\n",
              "  {'id': 92,\n",
              "   'seek': 53452,\n",
              "   'start': 553.8,\n",
              "   'end': 559.0799999999999,\n",
              "   'text': \" Python client I'm going to add a metadata field to the data frame and create two versions of the\",\n",
              "   'tokens': [51328,\n",
              "    15329,\n",
              "    6423,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    909,\n",
              "    257,\n",
              "    26603,\n",
              "    2519,\n",
              "    281,\n",
              "    264,\n",
              "    1412,\n",
              "    3920,\n",
              "    293,\n",
              "    1884,\n",
              "    732,\n",
              "    9606,\n",
              "    295,\n",
              "    264,\n",
              "    51592],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.133624267578125,\n",
              "   'compression_ratio': 1.6506550218340612,\n",
              "   'no_speech_prob': 0.005359705537557602},\n",
              "  {'id': 93,\n",
              "   'seek': 55908,\n",
              "   'start': 559.08,\n",
              "   'end': 563.8000000000001,\n",
              "   'text': ' data frame. One for uploading and a local version for extracting the actual reviews.',\n",
              "   'tokens': [50364,\n",
              "    1412,\n",
              "    3920,\n",
              "    13,\n",
              "    1485,\n",
              "    337,\n",
              "    27301,\n",
              "    293,\n",
              "    257,\n",
              "    2654,\n",
              "    3037,\n",
              "    337,\n",
              "    49844,\n",
              "    264,\n",
              "    3539,\n",
              "    10229,\n",
              "    13,\n",
              "    50600],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17631027433607313,\n",
              "   'compression_ratio': 1.5538461538461539,\n",
              "   'no_speech_prob': 0.0004875719896517694},\n",
              "  {'id': 94,\n",
              "   'seek': 55908,\n",
              "   'start': 565.08,\n",
              "   'end': 569.88,\n",
              "   'text': \" Then I'm going to create the Pinecon index directly with the Pinecon Python client.\",\n",
              "   'tokens': [50664,\n",
              "    1396,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    1884,\n",
              "    264,\n",
              "    33531,\n",
              "    1671,\n",
              "    8186,\n",
              "    3838,\n",
              "    365,\n",
              "    264,\n",
              "    33531,\n",
              "    1671,\n",
              "    15329,\n",
              "    6423,\n",
              "    13,\n",
              "    50904],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17631027433607313,\n",
              "   'compression_ratio': 1.5538461538461539,\n",
              "   'no_speech_prob': 0.0004875719896517694},\n",
              "  {'id': 95,\n",
              "   'seek': 55908,\n",
              "   'start': 571.0,\n",
              "   'end': 576.0400000000001,\n",
              "   'text': ' And while the index is initializing we can head over to the Pinecon documentation and have a',\n",
              "   'tokens': [50960,\n",
              "    400,\n",
              "    1339,\n",
              "    264,\n",
              "    8186,\n",
              "    307,\n",
              "    5883,\n",
              "    3319,\n",
              "    321,\n",
              "    393,\n",
              "    1378,\n",
              "    670,\n",
              "    281,\n",
              "    264,\n",
              "    33531,\n",
              "    1671,\n",
              "    14333,\n",
              "    293,\n",
              "    362,\n",
              "    257,\n",
              "    51212],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17631027433607313,\n",
              "   'compression_ratio': 1.5538461538461539,\n",
              "   'no_speech_prob': 0.0004875719896517694},\n",
              "  {'id': 96,\n",
              "   'seek': 55908,\n",
              "   'start': 576.0400000000001,\n",
              "   'end': 578.5200000000001,\n",
              "   'text': ' look at what the absurd should look like.',\n",
              "   'tokens': [51212, 574, 412, 437, 264, 19774, 820, 574, 411, 13, 51336],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.17631027433607313,\n",
              "   'compression_ratio': 1.5538461538461539,\n",
              "   'no_speech_prob': 0.0004875719896517694},\n",
              "  {'id': 97,\n",
              "   'seek': 57852,\n",
              "   'start': 579.3199999999999,\n",
              "   'end': 589.16,\n",
              "   'text': \" So we can see that we need an ID field and a value field with the vector values and then we're\",\n",
              "   'tokens': [50404,\n",
              "    407,\n",
              "    321,\n",
              "    393,\n",
              "    536,\n",
              "    300,\n",
              "    321,\n",
              "    643,\n",
              "    364,\n",
              "    7348,\n",
              "    2519,\n",
              "    293,\n",
              "    257,\n",
              "    2158,\n",
              "    2519,\n",
              "    365,\n",
              "    264,\n",
              "    8062,\n",
              "    4190,\n",
              "    293,\n",
              "    550,\n",
              "    321,\n",
              "    434,\n",
              "    50896],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13887936274210613,\n",
              "   'compression_ratio': 1.5477707006369428,\n",
              "   'no_speech_prob': 0.0002691822010092437},\n",
              "  {'id': 98,\n",
              "   'seek': 57852,\n",
              "   'start': 589.16,\n",
              "   'end': 592.6,\n",
              "   'text': ' going to add the metadata field used for filtering.',\n",
              "   'tokens': [50896,\n",
              "    516,\n",
              "    281,\n",
              "    909,\n",
              "    264,\n",
              "    26603,\n",
              "    2519,\n",
              "    1143,\n",
              "    337,\n",
              "    30822,\n",
              "    13,\n",
              "    51068],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13887936274210613,\n",
              "   'compression_ratio': 1.5477707006369428,\n",
              "   'no_speech_prob': 0.0002691822010092437},\n",
              "  {'id': 99,\n",
              "   'seek': 57852,\n",
              "   'start': 599.4,\n",
              "   'end': 604.52,\n",
              "   'text': \" Once the index has been initialized we can upload the data in patches and here I'm using patches\",\n",
              "   'tokens': [51408,\n",
              "    3443,\n",
              "    264,\n",
              "    8186,\n",
              "    575,\n",
              "    668,\n",
              "    5883,\n",
              "    1602,\n",
              "    321,\n",
              "    393,\n",
              "    6580,\n",
              "    264,\n",
              "    1412,\n",
              "    294,\n",
              "    26531,\n",
              "    293,\n",
              "    510,\n",
              "    286,\n",
              "    478,\n",
              "    1228,\n",
              "    26531,\n",
              "    51664],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.13887936274210613,\n",
              "   'compression_ratio': 1.5477707006369428,\n",
              "   'no_speech_prob': 0.0002691822010092437},\n",
              "  {'id': 100,\n",
              "   'seek': 60452,\n",
              "   'start': 604.52,\n",
              "   'end': 617.4,\n",
              "   'text': \" of 50. All right now that the upload is done let's try to run a filtered query.\",\n",
              "   'tokens': [50364,\n",
              "    295,\n",
              "    2625,\n",
              "    13,\n",
              "    1057,\n",
              "    558,\n",
              "    586,\n",
              "    300,\n",
              "    264,\n",
              "    6580,\n",
              "    307,\n",
              "    1096,\n",
              "    718,\n",
              "    311,\n",
              "    853,\n",
              "    281,\n",
              "    1190,\n",
              "    257,\n",
              "    37111,\n",
              "    14581,\n",
              "    13,\n",
              "    51008],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12240593869921187,\n",
              "   'compression_ratio': 1.4728260869565217,\n",
              "   'no_speech_prob': 0.003530124668031931},\n",
              "  {'id': 101,\n",
              "   'seek': 60452,\n",
              "   'start': 618.92,\n",
              "   'end': 625.64,\n",
              "   'text': ' We do that by using a query string like we do without the filter and then add a filter that has',\n",
              "   'tokens': [51084,\n",
              "    492,\n",
              "    360,\n",
              "    300,\n",
              "    538,\n",
              "    1228,\n",
              "    257,\n",
              "    14581,\n",
              "    6798,\n",
              "    411,\n",
              "    321,\n",
              "    360,\n",
              "    1553,\n",
              "    264,\n",
              "    6608,\n",
              "    293,\n",
              "    550,\n",
              "    909,\n",
              "    257,\n",
              "    6608,\n",
              "    300,\n",
              "    575,\n",
              "    51420],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12240593869921187,\n",
              "   'compression_ratio': 1.4728260869565217,\n",
              "   'no_speech_prob': 0.003530124668031931},\n",
              "  {'id': 102,\n",
              "   'seek': 60452,\n",
              "   'start': 625.64,\n",
              "   'end': 633.16,\n",
              "   'text': ' a MongoDB like syntax. This query will give us the top 100 matching reviews with a rating of 4.',\n",
              "   'tokens': [51420,\n",
              "    257,\n",
              "    48380,\n",
              "    27735,\n",
              "    411,\n",
              "    28431,\n",
              "    13,\n",
              "    639,\n",
              "    14581,\n",
              "    486,\n",
              "    976,\n",
              "    505,\n",
              "    264,\n",
              "    1192,\n",
              "    2319,\n",
              "    14324,\n",
              "    10229,\n",
              "    365,\n",
              "    257,\n",
              "    10990,\n",
              "    295,\n",
              "    1017,\n",
              "    13,\n",
              "    51796],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.12240593869921187,\n",
              "   'compression_ratio': 1.4728260869565217,\n",
              "   'no_speech_prob': 0.003530124668031931},\n",
              "  {'id': 103,\n",
              "   'seek': 63316,\n",
              "   'start': 634.12,\n",
              "   'end': 638.8399999999999,\n",
              "   'text': ' And the result is a list of ITs with a score indicating the match rate.',\n",
              "   'tokens': [50412,\n",
              "    400,\n",
              "    264,\n",
              "    1874,\n",
              "    307,\n",
              "    257,\n",
              "    1329,\n",
              "    295,\n",
              "    6783,\n",
              "    82,\n",
              "    365,\n",
              "    257,\n",
              "    6175,\n",
              "    25604,\n",
              "    264,\n",
              "    2995,\n",
              "    3314,\n",
              "    13,\n",
              "    50648],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08370903705028777,\n",
              "   'compression_ratio': 1.7098214285714286,\n",
              "   'no_speech_prob': 0.0007670139311812818},\n",
              "  {'id': 104,\n",
              "   'seek': 63316,\n",
              "   'start': 638.8399999999999,\n",
              "   'end': 642.76,\n",
              "   'text': \" To get the actual reviews we'll need to use the local version of the data.\",\n",
              "   'tokens': [50648,\n",
              "    1407,\n",
              "    483,\n",
              "    264,\n",
              "    3539,\n",
              "    10229,\n",
              "    321,\n",
              "    603,\n",
              "    643,\n",
              "    281,\n",
              "    764,\n",
              "    264,\n",
              "    2654,\n",
              "    3037,\n",
              "    295,\n",
              "    264,\n",
              "    1412,\n",
              "    13,\n",
              "    50844],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08370903705028777,\n",
              "   'compression_ratio': 1.7098214285714286,\n",
              "   'no_speech_prob': 0.0007670139311812818},\n",
              "  {'id': 105,\n",
              "   'seek': 63316,\n",
              "   'start': 644.92,\n",
              "   'end': 649.7199999999999,\n",
              "   'text': \" And what I'm going to do is I'm going to wrap the query in a Python function that executes\",\n",
              "   'tokens': [50952,\n",
              "    400,\n",
              "    437,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    360,\n",
              "    307,\n",
              "    286,\n",
              "    478,\n",
              "    516,\n",
              "    281,\n",
              "    7019,\n",
              "    264,\n",
              "    14581,\n",
              "    294,\n",
              "    257,\n",
              "    15329,\n",
              "    2445,\n",
              "    300,\n",
              "    4454,\n",
              "    1819,\n",
              "    51192],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08370903705028777,\n",
              "   'compression_ratio': 1.7098214285714286,\n",
              "   'no_speech_prob': 0.0007670139311812818},\n",
              "  {'id': 106,\n",
              "   'seek': 63316,\n",
              "   'start': 649.7199999999999,\n",
              "   'end': 655.88,\n",
              "   'text': ' a filtered vector similarity search followed by looping through the local version of the data',\n",
              "   'tokens': [51192,\n",
              "    257,\n",
              "    37111,\n",
              "    8062,\n",
              "    32194,\n",
              "    3164,\n",
              "    6263,\n",
              "    538,\n",
              "    6367,\n",
              "    278,\n",
              "    807,\n",
              "    264,\n",
              "    2654,\n",
              "    3037,\n",
              "    295,\n",
              "    264,\n",
              "    1412,\n",
              "    51500],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08370903705028777,\n",
              "   'compression_ratio': 1.7098214285714286,\n",
              "   'no_speech_prob': 0.0007670139311812818},\n",
              "  {'id': 107,\n",
              "   'seek': 63316,\n",
              "   'start': 655.88,\n",
              "   'end': 658.28,\n",
              "   'text': ' and extracting all the reviews from the local data.',\n",
              "   'tokens': [51500,\n",
              "    293,\n",
              "    49844,\n",
              "    439,\n",
              "    264,\n",
              "    10229,\n",
              "    490,\n",
              "    264,\n",
              "    2654,\n",
              "    1412,\n",
              "    13,\n",
              "    51620],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.08370903705028777,\n",
              "   'compression_ratio': 1.7098214285714286,\n",
              "   'no_speech_prob': 0.0007670139311812818},\n",
              "  {'id': 108,\n",
              "   'seek': 65828,\n",
              "   'start': 658.6,\n",
              "   'end': 665.64,\n",
              "   'text': ' Now if I call this function with the query will purchase again filtering out the ratings with',\n",
              "   'tokens': [50380,\n",
              "    823,\n",
              "    498,\n",
              "    286,\n",
              "    818,\n",
              "    341,\n",
              "    2445,\n",
              "    365,\n",
              "    264,\n",
              "    14581,\n",
              "    486,\n",
              "    8110,\n",
              "    797,\n",
              "    30822,\n",
              "    484,\n",
              "    264,\n",
              "    24603,\n",
              "    365,\n",
              "    50732],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19637645583554922,\n",
              "   'compression_ratio': 1.7511520737327189,\n",
              "   'no_speech_prob': 0.0006159691838547587},\n",
              "  {'id': 109,\n",
              "   'seek': 65828,\n",
              "   'start': 665.64,\n",
              "   'end': 671.24,\n",
              "   'text': ' five star reviews then we get all the happy customers that should be put in a repeat purchase flow.',\n",
              "   'tokens': [50732,\n",
              "    1732,\n",
              "    3543,\n",
              "    10229,\n",
              "    550,\n",
              "    321,\n",
              "    483,\n",
              "    439,\n",
              "    264,\n",
              "    2055,\n",
              "    4581,\n",
              "    300,\n",
              "    820,\n",
              "    312,\n",
              "    829,\n",
              "    294,\n",
              "    257,\n",
              "    7149,\n",
              "    8110,\n",
              "    3095,\n",
              "    13,\n",
              "    51012],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19637645583554922,\n",
              "   'compression_ratio': 1.7511520737327189,\n",
              "   'no_speech_prob': 0.0006159691838547587},\n",
              "  {'id': 110,\n",
              "   'seek': 65828,\n",
              "   'start': 673.24,\n",
              "   'end': 677.48,\n",
              "   'text': ' And we can do something similar to get all the disappointed customers that give a one star',\n",
              "   'tokens': [51112,\n",
              "    400,\n",
              "    321,\n",
              "    393,\n",
              "    360,\n",
              "    746,\n",
              "    2531,\n",
              "    281,\n",
              "    483,\n",
              "    439,\n",
              "    264,\n",
              "    13856,\n",
              "    4581,\n",
              "    300,\n",
              "    976,\n",
              "    257,\n",
              "    472,\n",
              "    3543,\n",
              "    51324],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19637645583554922,\n",
              "   'compression_ratio': 1.7511520737327189,\n",
              "   'no_speech_prob': 0.0006159691838547587},\n",
              "  {'id': 111,\n",
              "   'seek': 65828,\n",
              "   'start': 677.48,\n",
              "   'end': 684.8399999999999,\n",
              "   'text': ' review which works even though I misspelled disappointed and we can use this in a winback flow.',\n",
              "   'tokens': [51324,\n",
              "    3131,\n",
              "    597,\n",
              "    1985,\n",
              "    754,\n",
              "    1673,\n",
              "    286,\n",
              "    1713,\n",
              "    33000,\n",
              "    13856,\n",
              "    293,\n",
              "    321,\n",
              "    393,\n",
              "    764,\n",
              "    341,\n",
              "    294,\n",
              "    257,\n",
              "    1942,\n",
              "    3207,\n",
              "    3095,\n",
              "    13,\n",
              "    51692],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.19637645583554922,\n",
              "   'compression_ratio': 1.7511520737327189,\n",
              "   'no_speech_prob': 0.0006159691838547587},\n",
              "  {'id': 112,\n",
              "   'seek': 68828,\n",
              "   'start': 688.36,\n",
              "   'end': 693.3199999999999,\n",
              "   'text': ' Being able to filter out reviews like this is very powerful as it allows you to divide',\n",
              "   'tokens': [50368,\n",
              "    8891,\n",
              "    1075,\n",
              "    281,\n",
              "    6608,\n",
              "    484,\n",
              "    10229,\n",
              "    411,\n",
              "    341,\n",
              "    307,\n",
              "    588,\n",
              "    4005,\n",
              "    382,\n",
              "    309,\n",
              "    4045,\n",
              "    291,\n",
              "    281,\n",
              "    9845,\n",
              "    50616],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16594102210605266,\n",
              "   'compression_ratio': 1.7469879518072289,\n",
              "   'no_speech_prob': 0.0021476219408214092},\n",
              "  {'id': 113,\n",
              "   'seek': 68828,\n",
              "   'start': 693.3199999999999,\n",
              "   'end': 697.9599999999999,\n",
              "   'text': ' reviews into themes and get the angles you need in your remarketing campaigns.',\n",
              "   'tokens': [50616,\n",
              "    10229,\n",
              "    666,\n",
              "    13544,\n",
              "    293,\n",
              "    483,\n",
              "    264,\n",
              "    14708,\n",
              "    291,\n",
              "    643,\n",
              "    294,\n",
              "    428,\n",
              "    7942,\n",
              "    9880,\n",
              "    16840,\n",
              "    13,\n",
              "    50848],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16594102210605266,\n",
              "   'compression_ratio': 1.7469879518072289,\n",
              "   'no_speech_prob': 0.0021476219408214092},\n",
              "  {'id': 114,\n",
              "   'seek': 68828,\n",
              "   'start': 699.0,\n",
              "   'end': 703.72,\n",
              "   'text': ' In the example we just saw on the notebook we have two different themes we have the disappointed',\n",
              "   'tokens': [50900,\n",
              "    682,\n",
              "    264,\n",
              "    1365,\n",
              "    321,\n",
              "    445,\n",
              "    1866,\n",
              "    322,\n",
              "    264,\n",
              "    21060,\n",
              "    321,\n",
              "    362,\n",
              "    732,\n",
              "    819,\n",
              "    13544,\n",
              "    321,\n",
              "    362,\n",
              "    264,\n",
              "    13856,\n",
              "    51136],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16594102210605266,\n",
              "   'compression_ratio': 1.7469879518072289,\n",
              "   'no_speech_prob': 0.0021476219408214092},\n",
              "  {'id': 115,\n",
              "   'seek': 68828,\n",
              "   'start': 703.72,\n",
              "   'end': 709.9599999999999,\n",
              "   'text': ' customers and we have the customers that want to purchase again. And if you have a custom ID',\n",
              "   'tokens': [51136,\n",
              "    4581,\n",
              "    293,\n",
              "    321,\n",
              "    362,\n",
              "    264,\n",
              "    4581,\n",
              "    300,\n",
              "    528,\n",
              "    281,\n",
              "    8110,\n",
              "    797,\n",
              "    13,\n",
              "    400,\n",
              "    498,\n",
              "    291,\n",
              "    362,\n",
              "    257,\n",
              "    2375,\n",
              "    7348,\n",
              "    51448],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16594102210605266,\n",
              "   'compression_ratio': 1.7469879518072289,\n",
              "   'no_speech_prob': 0.0021476219408214092},\n",
              "  {'id': 116,\n",
              "   'seek': 68828,\n",
              "   'start': 709.9599999999999,\n",
              "   'end': 715.16,\n",
              "   'text': ' and an email tied to the customer review you can use Lanchions integration with',\n",
              "   'tokens': [51448,\n",
              "    293,\n",
              "    364,\n",
              "    3796,\n",
              "    9601,\n",
              "    281,\n",
              "    264,\n",
              "    5474,\n",
              "    3131,\n",
              "    291,\n",
              "    393,\n",
              "    764,\n",
              "    441,\n",
              "    4778,\n",
              "    626,\n",
              "    10980,\n",
              "    365,\n",
              "    51708],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.16594102210605266,\n",
              "   'compression_ratio': 1.7469879518072289,\n",
              "   'no_speech_prob': 0.0021476219408214092},\n",
              "  {'id': 117,\n",
              "   'seek': 71516,\n",
              "   'start': 715.16,\n",
              "   'end': 723.3199999999999,\n",
              "   'text': ' Sepia to create two lists a winback list and a repurchase list then use your email service provider',\n",
              "   'tokens': [50364,\n",
              "    22012,\n",
              "    654,\n",
              "    281,\n",
              "    1884,\n",
              "    732,\n",
              "    14511,\n",
              "    257,\n",
              "    1942,\n",
              "    3207,\n",
              "    1329,\n",
              "    293,\n",
              "    257,\n",
              "    1085,\n",
              "    2476,\n",
              "    651,\n",
              "    1329,\n",
              "    550,\n",
              "    764,\n",
              "    428,\n",
              "    3796,\n",
              "    2643,\n",
              "    12398,\n",
              "    50772],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18301118910312653,\n",
              "   'compression_ratio': 1.5321637426900585,\n",
              "   'no_speech_prob': 0.000127253879327327},\n",
              "  {'id': 118,\n",
              "   'seek': 71516,\n",
              "   'start': 723.3199999999999,\n",
              "   'end': 727.88,\n",
              "   'text': \" to run two different campaigns. That's a topic for another video.\",\n",
              "   'tokens': [50772,\n",
              "    281,\n",
              "    1190,\n",
              "    732,\n",
              "    819,\n",
              "    16840,\n",
              "    13,\n",
              "    663,\n",
              "    311,\n",
              "    257,\n",
              "    4829,\n",
              "    337,\n",
              "    1071,\n",
              "    960,\n",
              "    13,\n",
              "    51000],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18301118910312653,\n",
              "   'compression_ratio': 1.5321637426900585,\n",
              "   'no_speech_prob': 0.000127253879327327},\n",
              "  {'id': 119,\n",
              "   'seek': 71516,\n",
              "   'start': 729.16,\n",
              "   'end': 734.8399999999999,\n",
              "   'text': \" So that's it for now if you enjoyed this video give it a like and subscribe thanks for watching.\",\n",
              "   'tokens': [51064,\n",
              "    407,\n",
              "    300,\n",
              "    311,\n",
              "    309,\n",
              "    337,\n",
              "    586,\n",
              "    498,\n",
              "    291,\n",
              "    4626,\n",
              "    341,\n",
              "    960,\n",
              "    976,\n",
              "    309,\n",
              "    257,\n",
              "    411,\n",
              "    293,\n",
              "    3022,\n",
              "    3231,\n",
              "    337,\n",
              "    1976,\n",
              "    13,\n",
              "    51348],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.18301118910312653,\n",
              "   'compression_ratio': 1.5321637426900585,\n",
              "   'no_speech_prob': 0.000127253879327327}],\n",
              " 'language': 'en'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = transcription['text']"
      ],
      "metadata": {
        "id": "HS9GCRIQKogF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P2rapCXRPge",
        "outputId": "0548828b-ed1a-4329-8b72-c3eb6f18732d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " At the heart of the language model revolution and the chain framework lies the concept of a text embedding. A text embedding is a learned representation of text that takes the form of a vector of numbers. This vector allows us to efficiently prompt and retrieve context from vector storage to extract relevant pieces of information, enhance the language models memory and capabilities and ultimately take the action we want to take to generate value. In this video, we're going to look at this process by means of a real world practical application. We are going to use Langchain to extract information and value from Amazon Review Data. One of the most slam dunk applications of Langchain is custom experience and analytics. I'm going to show you how you can take the unstructured review data and map the reviews into themes and a structure that allows you to act on the data. I'm also going to demonstrate how the review embeddings can form the basis as inputs to other machine learning models and just how packed these reviews are with signal that can be used to further enhance the capabilities of the language models and avenue we're going to explore in more detail going forward on this channel. By the end of this video, you'll have an idea of how you can generate value for businesses with Langchain, how you combine vector stores with large language models and you'll have a small POC that you can further build on and put to good use. To get started, we'll pip install the needed libraries and we'll drop the needed API keys in variables into an environment file. Links to the code and the data will be available below the video. Let's go. So here we have the Amazon data that we're going to use. These are reviews for products in the fashion category. What's important to note here, there's an overall rating. There is a review text with the actual review from the customer and there's an ID that allows us to try this to a specific product and then we have a complimentary data set called meta-amazon fashion with all the product information and an ID that allows us to join this with the review data set. In our notebook, the first thing we're going to do is we're going to import the utilities needed for loading the API keys from the environment file and also extract the Amazon data from the files. And we're actually not going to extract the data from the JSON files as they are quite big, but directly from the SIPT files. Then I'm going to load both data sets into pandas data frames and I'm removing reviews without a review text as we need the text for the work in this video. And here we have the review data in a data frame. And this is the metadata also in a data frame. Next, I'm going to trunk it reviews so that we don't process reviews that are too long and you can play around with the number of characters you want to use. And then I'm going to find a product that has a good number of reviews for the sake of this video. And I had a look at the data and it looks like the second one from the bottom is a good fit. And if we extract that product, you can see the name it's called power step pinnacle, authentic shoe insoles. I guess that's fashion, but this is good for our purpose. So let's create the embedding vectors from these reviews. So we're going to work on just a slice of the data frame with the power step pinnacle insoles. And as for embeddings, I'm going to use hucking phase embeddings. And this is just to show that we don't have to use openAI for this. I'm going to create a new column in the data frame with the embedding vectors. So I'm going to use the apply function on the data frame and the word of caution here. Another reason I'm not using the openAI embeddings is because of the openAI pricing model. When you're running apply with an embedding function on a large data frame, you risk incurring significant costs using openAI. Here we're only working on a slice of the original full data frame, but the full data frame has more than 800,000 rows. So please don't run the apply function with an openAI embedding unless you know what you're doing. All right, here we have the embedding vectors as a new column in the data frame. And what I'm going to do now is I'm going to show you the richness of this review data. And I'll do that by training a simple random forest machine learning model with the embedding vectors as features and the overall rating as a target. I'll use scikit-learn to divide the data into a training sample and a test sample. And then I'm going to import a random forest regressor. So I'm treating this as a regression problem, meaning that the prediction is on a continuum, even though that we know that the rating is an integer. But this is fine for demonstration. And then I'm training the model on the training part of the data set. And here you can also play around with the number of estimators you're using. I found that 150 was fine to make this point. And once the fitting is done, we can evaluate by using the mean absolute error on the test part of the data set. And we see that we achieve a mean absolute error of 0.53, which means that on average, the prediction is off by around half a point. And this is with a simple non-optimized machine learning model that takes five minutes to run. If you spend some time optimizing this, are building a more advanced model in PyTorch, you could probably get to 0.3 or below. So there is significant signal in this data. But why would you want to predict the rating? Well, you wouldn't unless you're missing the rating for some of the reviews. What you want to do is to switch out the target variable and use the signal in the review data to build machine learning models that will actually help you generate value. And these are product recommendation models, churn or retention models, propensity models, uplift models, and so on. I'm now going to load the review embeddings into the vector database and show you how we can have GBT for access the data and give us a summary of the reviews. I'm using GBT for as it's currently the most powerful language model we have to work with, but you can switch out the language model as you see fit. To upload the review embeddings, we import and initiate pine cone and then transform the truncated review text column into a list of reviews. Then we upload the reviews with the built-in from text method using Hocking Face embeddings. Once the upload is done, we can head over to pine cone and check that the vectors have been uploaded. And here we have all the review vectors uploaded into pine cone. And then we can do a basic vector similarity search to check that everything works. What I want to do now is I want to have the language model access the data in the vector store. So I'm going to import retrieval QA that is used to retrieve the most relevant reviews given a prompt and feed those to the language model. And then I'm going to import chat.obmAPI that I'm going to use as a driver around GBT for. Then we're going to define a chain. It's called review chain using retrieval QA that takes the language model, the vector store, and then the chain type as an argument. And here we use chain type stuff, which means that we stuff all the related data into the prompt and we use that as a context and pass it to the language model. Then we simply write the query as we usually do when working with chat.obt and we run the chain with this query. And here I'm asking to be for to give us an overall impression of the reviews and give us the most prevalent examples and bullet points and also give us suggestions for improvement. And remember when you're working with chat models like DBT4 you can send system messages that will allow you to calibrate the system of the model, which could significantly improve the quality of the output. Alright so here we are with DBT4's impression of the reviews. Some examples of good reviews, mixed reviews, some negative reviews, and then based on those reviews there are some potential areas to focus on improving. So pretty nice and I'm sure you can improve that by calibrating the system. What's really powerful about this is that you can turn this into a weekly digest and send it out to an internal email list within a company and this can also be done with chain. Alright so the last thing we're going to have a look at is filtered vector similar research and what I'd like to do is to get all the reviews of a given rating that match a specific theme so we need to filter on metadata to do that. I'm going to use Pinecon directly as Langchain doesn't seem to support this out of the box yet. To upload the data with the Pinecon Python client I'm going to add a metadata field to the data frame and create two versions of the data frame. One for uploading and a local version for extracting the actual reviews. Then I'm going to create the Pinecon index directly with the Pinecon Python client. And while the index is initializing we can head over to the Pinecon documentation and have a look at what the absurd should look like. So we can see that we need an ID field and a value field with the vector values and then we're going to add the metadata field used for filtering. Once the index has been initialized we can upload the data in patches and here I'm using patches of 50. All right now that the upload is done let's try to run a filtered query. We do that by using a query string like we do without the filter and then add a filter that has a MongoDB like syntax. This query will give us the top 100 matching reviews with a rating of 4. And the result is a list of ITs with a score indicating the match rate. To get the actual reviews we'll need to use the local version of the data. And what I'm going to do is I'm going to wrap the query in a Python function that executes a filtered vector similarity search followed by looping through the local version of the data and extracting all the reviews from the local data. Now if I call this function with the query will purchase again filtering out the ratings with five star reviews then we get all the happy customers that should be put in a repeat purchase flow. And we can do something similar to get all the disappointed customers that give a one star review which works even though I misspelled disappointed and we can use this in a winback flow. Being able to filter out reviews like this is very powerful as it allows you to divide reviews into themes and get the angles you need in your remarketing campaigns. In the example we just saw on the notebook we have two different themes we have the disappointed customers and we have the customers that want to purchase again. And if you have a custom ID and an email tied to the customer review you can use Lanchions integration with Sepia to create two lists a winback list and a repurchase list then use your email service provider to run two different campaigns. That's a topic for another video. So that's it for now if you enjoyed this video give it a like and subscribe thanks for watching.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def store_segments(segments):\n",
        "  texts = []\n",
        "  start_times = []\n",
        "\n",
        "  for segment in segments:\n",
        "    text = segment['text']\n",
        "    start = segment['start']\n",
        "\n",
        "    # Convert the starting time to a datetime object\n",
        "    start_datetime = datetime.fromtimestamp(start)\n",
        "\n",
        "    # Format the starting time as a string in the format \"00:00:00\"\n",
        "    formatted_start_time = start_datetime.strftime('%H:%M:%S')\n",
        "\n",
        "    texts.append(\"\".join(text))\n",
        "    start_times.append(formatted_start_time)\n",
        "\n",
        "  return texts, start_times"
      ],
      "metadata": {
        "id": "lHVWts6YKuDq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments = transcription['segments']"
      ],
      "metadata": {
        "id": "QAzfMtFkcnNp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_segments(segments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4IaERsPKxKu",
        "outputId": "e344f9a1-c882-4ee3-f88e-7bf3f6940803"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([' At the heart of the language model revolution and the chain framework lies the concept of a text embedding.',\n",
              "  ' A text embedding is a learned representation of text that takes the form of a vector of numbers.',\n",
              "  ' This vector allows us to efficiently prompt and retrieve context from vector storage',\n",
              "  ' to extract relevant pieces of information, enhance the language models memory and capabilities',\n",
              "  ' and ultimately take the action we want to take to generate value.',\n",
              "  \" In this video, we're going to look at this process by means of a real world practical application.\",\n",
              "  ' We are going to use Langchain to extract information and value from Amazon Review Data.',\n",
              "  ' One of the most slam dunk applications of Langchain is custom experience and analytics.',\n",
              "  \" I'm going to show you how you can take the unstructured review data and map the reviews into themes\",\n",
              "  ' and a structure that allows you to act on the data.',\n",
              "  \" I'm also going to demonstrate how the review embeddings can form the basis as inputs\",\n",
              "  ' to other machine learning models and just how packed these reviews are with signal',\n",
              "  \" that can be used to further enhance the capabilities of the language models and avenue we're going to\",\n",
              "  \" explore in more detail going forward on this channel. By the end of this video, you'll have an idea\",\n",
              "  ' of how you can generate value for businesses with Langchain, how you combine vector stores with',\n",
              "  \" large language models and you'll have a small POC that you can further build on and put to good use.\",\n",
              "  \" To get started, we'll pip install the needed libraries and we'll drop the needed API keys in\",\n",
              "  ' variables into an environment file. Links to the code and the data will be available below the video.',\n",
              "  \" Let's go. So here we have the Amazon data that we're going to use. These are reviews for products\",\n",
              "  \" in the fashion category. What's important to note here, there's an overall rating. There is a\",\n",
              "  \" review text with the actual review from the customer and there's an ID that allows us to\",\n",
              "  ' try this to a specific product and then we have a complimentary data set called meta-amazon fashion',\n",
              "  ' with all the product information and an ID that allows us to join this with the review data set.',\n",
              "  \" In our notebook, the first thing we're going to do is we're going to import the utilities needed\",\n",
              "  ' for loading the API keys from the environment file and also extract the Amazon data from the files.',\n",
              "  \" And we're actually not going to extract the data from the JSON files as they are quite big,\",\n",
              "  \" but directly from the SIPT files. Then I'm going to load both data sets into pandas data frames\",\n",
              "  \" and I'm removing reviews without a review text as we need the text for the work in this video.\",\n",
              "  ' And here we have the review data in a data frame.',\n",
              "  ' And this is the metadata also in a data frame.',\n",
              "  \" Next, I'm going to trunk it reviews so that we don't process reviews that are too long and you\",\n",
              "  \" can play around with the number of characters you want to use. And then I'm going to find a\",\n",
              "  ' product that has a good number of reviews for the sake of this video. And I had a look at the',\n",
              "  ' data and it looks like the second one from the bottom is a good fit. And if we extract that product,',\n",
              "  \" you can see the name it's called power step pinnacle, authentic shoe insoles. I guess that's\",\n",
              "  \" fashion, but this is good for our purpose. So let's create the embedding vectors from these reviews.\",\n",
              "  \" So we're going to work on just a slice of the data frame with the power step pinnacle insoles.\",\n",
              "  \" And as for embeddings, I'm going to use hucking phase embeddings. And this is just to show that\",\n",
              "  \" we don't have to use openAI for this. I'm going to create a new column in the data frame with the\",\n",
              "  \" embedding vectors. So I'm going to use the apply function on the data frame and the word of\",\n",
              "  \" caution here. Another reason I'm not using the openAI embeddings is because of the openAI pricing\",\n",
              "  \" model. When you're running apply with an embedding function on a large data frame, you risk incurring\",\n",
              "  \" significant costs using openAI. Here we're only working on a slice of the original full data frame,\",\n",
              "  \" but the full data frame has more than 800,000 rows. So please don't run the apply function with an\",\n",
              "  \" openAI embedding unless you know what you're doing. All right, here we have the embedding vectors\",\n",
              "  \" as a new column in the data frame. And what I'm going to do now is I'm going to show you the\",\n",
              "  \" richness of this review data. And I'll do that by training a simple random forest machine learning\",\n",
              "  \" model with the embedding vectors as features and the overall rating as a target. I'll use scikit-learn\",\n",
              "  \" to divide the data into a training sample and a test sample. And then I'm going to import a random\",\n",
              "  \" forest regressor. So I'm treating this as a regression problem, meaning that the prediction is\",\n",
              "  ' on a continuum, even though that we know that the rating is an integer. But this is fine for',\n",
              "  \" demonstration. And then I'm training the model on the training part of the data set. And here you\",\n",
              "  \" can also play around with the number of estimators you're using. I found that 150 was fine to make this\",\n",
              "  ' point. And once the fitting is done, we can evaluate by using the mean absolute error on the test',\n",
              "  ' part of the data set. And we see that we achieve a mean absolute error of 0.53, which means that',\n",
              "  ' on average, the prediction is off by around half a point. And this is with a simple non-optimized',\n",
              "  ' machine learning model that takes five minutes to run. If you spend some time optimizing this,',\n",
              "  ' are building a more advanced model in PyTorch, you could probably get to 0.3 or below. So there',\n",
              "  \" is significant signal in this data. But why would you want to predict the rating? Well, you wouldn't\",\n",
              "  \" unless you're missing the rating for some of the reviews. What you want to do is to switch out\",\n",
              "  ' the target variable and use the signal in the review data to build machine learning models that',\n",
              "  ' will actually help you generate value. And these are product recommendation models, churn or retention',\n",
              "  \" models, propensity models, uplift models, and so on. I'm now going to load the review embeddings\",\n",
              "  ' into the vector database and show you how we can have GBT for access the data and give us a summary',\n",
              "  \" of the reviews. I'm using GBT for as it's currently the most powerful language model we have to\",\n",
              "  ' work with, but you can switch out the language model as you see fit. To upload the review embeddings,',\n",
              "  ' we import and initiate pine cone and then transform the truncated review text column into a list of reviews.',\n",
              "  ' Then we upload the reviews with the built-in from text method using Hocking Face embeddings.',\n",
              "  ' Once the upload is done, we can head over to pine cone and check that the vectors have been uploaded.',\n",
              "  ' And here we have all the review vectors uploaded into pine cone.',\n",
              "  ' And then we can do a basic vector similarity search to check that everything works.',\n",
              "  ' What I want to do now is I want to have the language model access the data in the vector store.',\n",
              "  \" So I'm going to import retrieval QA that is used to retrieve the most relevant reviews given\",\n",
              "  \" a prompt and feed those to the language model. And then I'm going to import chat.obmAPI that I'm\",\n",
              "  \" going to use as a driver around GBT for. Then we're going to define a chain. It's called review chain\",\n",
              "  ' using retrieval QA that takes the language model, the vector store, and then the chain type as an',\n",
              "  ' argument. And here we use chain type stuff, which means that we stuff all the related data into',\n",
              "  ' the prompt and we use that as a context and pass it to the language model. Then we simply write',\n",
              "  ' the query as we usually do when working with chat.obt and we run the chain with this query.',\n",
              "  \" And here I'm asking to be for to give us an overall impression of the reviews and give us the\",\n",
              "  ' most prevalent examples and bullet points and also give us suggestions for improvement.',\n",
              "  \" And remember when you're working with chat models like DBT4 you can send system messages that\",\n",
              "  ' will allow you to calibrate the system of the model, which could significantly improve the',\n",
              "  \" quality of the output. Alright so here we are with DBT4's impression of the reviews.\",\n",
              "  ' Some examples of good reviews, mixed reviews, some negative reviews, and then based on those',\n",
              "  \" reviews there are some potential areas to focus on improving. So pretty nice and I'm sure you can\",\n",
              "  \" improve that by calibrating the system. What's really powerful about this is that you can turn this\",\n",
              "  ' into a weekly digest and send it out to an internal email list within a company and this can also be',\n",
              "  \" done with chain. Alright so the last thing we're going to have a look at is filtered vector similar\",\n",
              "  \" research and what I'd like to do is to get all the reviews of a given rating that match a specific\",\n",
              "  \" theme so we need to filter on metadata to do that. I'm going to use Pinecon directly as\",\n",
              "  \" Langchain doesn't seem to support this out of the box yet. To upload the data with the Pinecon\",\n",
              "  \" Python client I'm going to add a metadata field to the data frame and create two versions of the\",\n",
              "  ' data frame. One for uploading and a local version for extracting the actual reviews.',\n",
              "  \" Then I'm going to create the Pinecon index directly with the Pinecon Python client.\",\n",
              "  ' And while the index is initializing we can head over to the Pinecon documentation and have a',\n",
              "  ' look at what the absurd should look like.',\n",
              "  \" So we can see that we need an ID field and a value field with the vector values and then we're\",\n",
              "  ' going to add the metadata field used for filtering.',\n",
              "  \" Once the index has been initialized we can upload the data in patches and here I'm using patches\",\n",
              "  \" of 50. All right now that the upload is done let's try to run a filtered query.\",\n",
              "  ' We do that by using a query string like we do without the filter and then add a filter that has',\n",
              "  ' a MongoDB like syntax. This query will give us the top 100 matching reviews with a rating of 4.',\n",
              "  ' And the result is a list of ITs with a score indicating the match rate.',\n",
              "  \" To get the actual reviews we'll need to use the local version of the data.\",\n",
              "  \" And what I'm going to do is I'm going to wrap the query in a Python function that executes\",\n",
              "  ' a filtered vector similarity search followed by looping through the local version of the data',\n",
              "  ' and extracting all the reviews from the local data.',\n",
              "  ' Now if I call this function with the query will purchase again filtering out the ratings with',\n",
              "  ' five star reviews then we get all the happy customers that should be put in a repeat purchase flow.',\n",
              "  ' And we can do something similar to get all the disappointed customers that give a one star',\n",
              "  ' review which works even though I misspelled disappointed and we can use this in a winback flow.',\n",
              "  ' Being able to filter out reviews like this is very powerful as it allows you to divide',\n",
              "  ' reviews into themes and get the angles you need in your remarketing campaigns.',\n",
              "  ' In the example we just saw on the notebook we have two different themes we have the disappointed',\n",
              "  ' customers and we have the customers that want to purchase again. And if you have a custom ID',\n",
              "  ' and an email tied to the customer review you can use Lanchions integration with',\n",
              "  ' Sepia to create two lists a winback list and a repurchase list then use your email service provider',\n",
              "  \" to run two different campaigns. That's a topic for another video.\",\n",
              "  \" So that's it for now if you enjoyed this video give it a like and subscribe thanks for watching.\"],\n",
              " ['00:00:00',\n",
              "  '00:00:06',\n",
              "  '00:00:11',\n",
              "  '00:00:17',\n",
              "  '00:00:22',\n",
              "  '00:00:26',\n",
              "  '00:00:31',\n",
              "  '00:00:36',\n",
              "  '00:00:41',\n",
              "  '00:00:46',\n",
              "  '00:00:50',\n",
              "  '00:00:54',\n",
              "  '00:00:59',\n",
              "  '00:01:04',\n",
              "  '00:01:10',\n",
              "  '00:01:16',\n",
              "  '00:01:22',\n",
              "  '00:01:28',\n",
              "  '00:01:34',\n",
              "  '00:01:41',\n",
              "  '00:01:48',\n",
              "  '00:01:52',\n",
              "  '00:01:59',\n",
              "  '00:02:07',\n",
              "  '00:02:11',\n",
              "  '00:02:18',\n",
              "  '00:02:22',\n",
              "  '00:02:29',\n",
              "  '00:02:34',\n",
              "  '00:02:44',\n",
              "  '00:02:51',\n",
              "  '00:02:56',\n",
              "  '00:03:01',\n",
              "  '00:03:06',\n",
              "  '00:03:13',\n",
              "  '00:03:19',\n",
              "  '00:03:27',\n",
              "  '00:03:34',\n",
              "  '00:03:39',\n",
              "  '00:03:46',\n",
              "  '00:03:52',\n",
              "  '00:03:59',\n",
              "  '00:04:05',\n",
              "  '00:04:12',\n",
              "  '00:04:19',\n",
              "  '00:04:25',\n",
              "  '00:04:30',\n",
              "  '00:04:36',\n",
              "  '00:04:43',\n",
              "  '00:04:48',\n",
              "  '00:04:54',\n",
              "  '00:04:59',\n",
              "  '00:05:05',\n",
              "  '00:05:10',\n",
              "  '00:05:17',\n",
              "  '00:05:24',\n",
              "  '00:05:30',\n",
              "  '00:05:35',\n",
              "  '00:05:40',\n",
              "  '00:05:45',\n",
              "  '00:05:50',\n",
              "  '00:05:56',\n",
              "  '00:06:02',\n",
              "  '00:06:09',\n",
              "  '00:06:15',\n",
              "  '00:06:20',\n",
              "  '00:06:26',\n",
              "  '00:06:38',\n",
              "  '00:06:45',\n",
              "  '00:06:52',\n",
              "  '00:07:00',\n",
              "  '00:07:06',\n",
              "  '00:07:10',\n",
              "  '00:07:15',\n",
              "  '00:07:21',\n",
              "  '00:07:28',\n",
              "  '00:07:35',\n",
              "  '00:07:40',\n",
              "  '00:07:45',\n",
              "  '00:07:53',\n",
              "  '00:07:58',\n",
              "  '00:08:04',\n",
              "  '00:08:09',\n",
              "  '00:08:15',\n",
              "  '00:08:22',\n",
              "  '00:08:28',\n",
              "  '00:08:35',\n",
              "  '00:08:42',\n",
              "  '00:08:48',\n",
              "  '00:08:54',\n",
              "  '00:09:00',\n",
              "  '00:09:07',\n",
              "  '00:09:13',\n",
              "  '00:09:19',\n",
              "  '00:09:25',\n",
              "  '00:09:31',\n",
              "  '00:09:36',\n",
              "  '00:09:39',\n",
              "  '00:09:49',\n",
              "  '00:09:59',\n",
              "  '00:10:04',\n",
              "  '00:10:18',\n",
              "  '00:10:25',\n",
              "  '00:10:34',\n",
              "  '00:10:38',\n",
              "  '00:10:44',\n",
              "  '00:10:49',\n",
              "  '00:10:55',\n",
              "  '00:10:58',\n",
              "  '00:11:05',\n",
              "  '00:11:13',\n",
              "  '00:11:17',\n",
              "  '00:11:28',\n",
              "  '00:11:33',\n",
              "  '00:11:39',\n",
              "  '00:11:43',\n",
              "  '00:11:49',\n",
              "  '00:11:55',\n",
              "  '00:12:03',\n",
              "  '00:12:09'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts, start_times = store_segments(segments)"
      ],
      "metadata": {
        "id": "__-hrw_6LYsu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "id": "7auAzAfXL1V7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c84f40-40c9-43ce-e641-7af0630d9064"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.183-py3-none-any.whl (938 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.183 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "EMG0TDLoL5rr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140148eb-230f-418d-a35b-4961ff67c4b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/72.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade faiss-gpu #==1.7.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TNi-J7FMAlT",
        "outputId": "b8893723-fe0c-4e92-bf03-535386a89758"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.chains import VectorDBQAWithSourcesChain\n",
        "from langchain.llms import AzureOpenAI\n",
        "import openai\n",
        "import faiss"
      ],
      "metadata": {
        "id": "sJvI6zAcLpDZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "OPENAI_API_KEY = getpass(\"OpenAI API Key: \")"
      ],
      "metadata": {
        "id": "gqpgTdzbdaEU",
        "outputId": "2077a07d-e86a-429b-80a7-4afd7a19710b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RG = input(\"Azure OpenAI resource group name: \")\n",
        "OPENAI_API_BASE = 'https://' + RG +'.openai.azure.com'"
      ],
      "metadata": {
        "id": "rp-xlMyZdffO",
        "outputId": "2d94a4a4-5a8f-4f86-8d9e-a3f152a7ba8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure OpenAI resource group name: silearnai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "# API version to use (Azure has several)\n",
        "os.environ['OPENAI_API_VERSION'] = '2023-03-15-preview' #or '2022-12-01'\n",
        "# base URL for your Azure OpenAI resource\n",
        "os.environ['OPENAI_API_BASE'] = OPENAI_API_BASE"
      ],
      "metadata": {
        "id": "eU40XsWdMHJH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t3U0N5dYdj6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\"\\n\")\n",
        "docs = []\n",
        "metadatas = []\n",
        "for i, d in enumerate(texts):\n",
        "    splits = text_splitter.split_text(d)\n",
        "    docs.extend(splits)\n",
        "    metadatas.extend([{\"source\": start_times[i]}] * len(splits))"
      ],
      "metadata": {
        "id": "6ZylCKSqMLm-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    deployment='embedding01',\n",
        "    model='text-embedding-ada-002',\n",
        "    chunk_size=1\n",
        ")"
      ],
      "metadata": {
        "id": "yOTyQ3TZewWc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = FAISS.from_texts(docs, embeddings, metadatas=metadatas)\n",
        "faiss.write_index(store.index, \"docs.index\")"
      ],
      "metadata": {
        "id": "VScsZ_MzMRuv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = openai = AzureOpenAI(\n",
        "    deployment_name=\"gpt301\", \n",
        "    model_name=\"text-davinci-003\"\n",
        ")  "
      ],
      "metadata": {
        "id": "_O3eHe1Ehvxf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = VectorDBQAWithSourcesChain.from_llm(llm=llm, vectorstore=store)"
      ],
      "metadata": {
        "id": "tk-fzMBgMt9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06a7a7e-da38-4e01-a5c7-2ec026a7dac4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/qa_with_sources/vector_db.py:60: UserWarning: `VectorDBQAWithSourcesChain` is deprecated - please use `from langchain.chains import RetrievalQAWithSourcesChain`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2aPjaLB_Gps6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9a7a9a-1845-41dd-c861-4b042be332b2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain({\"question\": \"How old was Steve Jobs when started Apple?\"})"
      ],
      "metadata": {
        "id": "8JUgPLpnNOvJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Answer: {result['answer']}  Sources: {result['sources']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du4gg4C1NX_h",
        "outputId": "2e60cfed-9f75-4fe2-bcfa-a0801816e380"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  I don't know.\n",
            "  Sources: 00:00:26, 00:10:04, 00:08:22, 00:05:05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain({\"question\": \"What are the use cases?\"})"
      ],
      "metadata": {
        "id": "svsWOrzzknb7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Answer: {result['answer']}  Sources: {result['sources']}\")"
      ],
      "metadata": {
        "id": "IsC7z9_JkwF1",
        "outputId": "4a4ff195-bead-4ac0-f9b4-0f8f9ccfb586",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  The use cases of Langchain are custom experience and analytics. \n",
            "  Sources: 00:00:36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain({\"question\": \"What are possible machine learning models?\"})\n",
        "print(f\"Answer: {result['answer']}  Sources: {result['sources']}\")"
      ],
      "metadata": {
        "id": "sSNtfSrZkxZ7",
        "outputId": "28509007-0f0d-4ba6-a4a7-c85afde6d105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  Possible machine learning models include a model with embedding vectors as features and overall rating as a target, propensity models, uplift models, and a random forest machine learning model.\n",
            "  Sources: 00:04:30, 00:04:36, 00:06:02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gyQ52nVlQTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}